%!TEX root = ../main.tex
% Chapitres/Chap3-UnOutilAideDecision

% ..............................................................................
% ..............................................................................
\section{Les méthodes d’aide à la décision} % (fold)
\label{sec:les_methodes_d_aide_à_la_decision}
% ------------------------------------------------------------------------------
\subsection{Introduction} % (fold)
\label{sub:introduction}
L’utilisation d’une méthode multi-critère d’aide à la décision (MCDA) fait toujours suite à une pré-analyse
permettant d’évaluer la ou les raisons motivant sa formulation. Elle peut être définie
comme le processus permettant d’obtenir des éléments de réponses ou des prescriptions
à partir de l’ensemble des informations disponibles. D’après \mtodo{Citation roy 1996}{Roy}
un problème peut être classé suivant trois problématiques, à savoir de choix, de trie,
ou de rangement. Dans les trois cas les différentes méthodes de MCDA nécessitent
le respect de quatre étapes :
\begin{itemize}
  \item Lister les actions potentielles
  \item Lister les critères à considérer
  \item Réaliser un tableau de performance
  \item Agréger les performances en un indicateur de décision
\end{itemize}
D’après \mtodo{Citation roy 1985}{Roy}, "une action 'a' est la représentation d’une
éventuelle contribution à la décision globale, susceptible, eu à l’égard à l’état
d’avancement du processus de décision, d’être envisagée de façon autonome et de
servir de point d’application à l’aide à la décision (ce point pouvant suffire à
caractériser a)". L’ensemble des actions sera ainsi évalué en fonction des
critères qui devront si possible êtres indépendants. La construction des critères
demande alors une connaissance importante du problème et implique fortement le
ou les décideurs. Les critères sont ainsi évalués au regard des actions, une pondération
et en fonction des méthodes une pondération peut être nécessaire pour évaluer la force
de la contribution de chaque action sur la décision finale. Il est important de noter
que le processus d’aide à la décision n’est pas linéaire et ces étapes peuvent
être répétés plusieurs fois.


À travers cette première étape, il est ainsi nécessaire de faire le bilan de la
performance actuelle, le résultat espéré, et les pistes permettant de définir a
priori comment ces outils peuvent apporter une réponse aux questionnements.
L’expérience et les résultats obtenues en amont permettent alors d’alimenter
la réflexion amenant à la formulation des critères et actions de manière précise
nécessaire pour sélectionner une méthode d’aide à la décision adaptée.
Lorsque le problème suggère une optimisation, il est nécessaire de définir les critères
à respecter. On parle alors des objectifs d’une optimisation multi-critère.
La définition des fonctions objectifs et des contraintes est propre à chaque problème
et est directement dépendant des données disponibles.
Dans le secteur du bâtiment, il est courant de considérer certaines sorties des
logiciels de simulation dynamique (STD, CFD, ...) comme objectifs.
\mtodo{Citation Attia et al. 2013}{Auteur} montre que dans le cas d’études sur la
performance des bâtiment passifs la consommation, le coût, et le confort son
couramment sélectionnés .
\itodo{Ajouter les objectifs courant sur les systèmes et couplage système / bâtiment}

Il est aussi nécessaire de définir les variables de décisions intéressantes a priori.
Les variables peuvent être classées en deux grandes familles : les quantitatives et
les qualitatives (Fig~\ref{fig:type_variable}).
Les variables quantitatives exprime une quantité à travers un nombre et
peuvent être discrètes ou continues. On considère ainsi respectivement un nombre de
valeurs discrètes (ex : épaisseur d’un isolant) ou une plage de variation définissant
ces limites/bornes (ex : épaisseur d’une dalle en béton).
Enfin, les variables qualitatives permettent de décrire une variation non ordonnable,
dite catégorielle (ex : couleur) ou bien floue (ex : beau / laid).

\begin{figure}
    \begin{center}
        \includegraphics{Ressources/Images/optimisation/type_variable.pdf}
    \end{center}
    \caption{Description des catégories existantes pour une variable.
             \label{fig:type_variable}}
\end{figure}

Des contraintes peuvent aussi être ajoutées afin de répondre aux exigences
techniques en se basant sur les connaissances à priori du problème ou bien dans
l’optique de limiter l’espace de recherche. On distingue deux types de contraintes.
La première définie a priori et dont l’impact n’est pas considéré dans le processus
d’aide à la décision (ex : surface disponible en toiture pour installer des
panneaux photovoltaïques).
Le second type est plus complexe et ne peut pas être pris en compte en amont de
l’analyse car des informations sont manquantes. On peut vouloir par exemple limiter
un objectif ou éviter des combinaisons non réalisables. Ces contraintes sont donc
parti intégrante de la méthode d’aide à la décision.
Elle peuvent s’exprimer sous la forme de bornes, d’équations ou inéquations.

La suite de cette section vise à présenter de manière succincte les différentes
approches existantes. Pour le lecteur intéressé, une introduction plus complète
des MCDA ainsi que de nombreuses références est proposé par (\mtodo{Ajouter citation}{Sami ben mena}).
Ensuite l’optimisation multi-objectif est décrite et finalement l’approche retenue
est détaillée à travers la description de l’algorithme.
% subsection introduction (end)

% ------------------------------------------------------------------------------
\subsection{Les approches existantes} % (fold)
\label{sub:les_approches_existantes}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Décision a priori} % (fold)
\label{ssub:decision_a_priori}
Dans cette approche, le problème multi-objectif est réduit à un problème mono-objectif.
Ce processus de réduction est réalisé à l’aide de méthodes d’agrégation parmi lesquelles
il est possible de citer les méthodes de pondération, de compromis,
ou encore de distance (Fig.~\ref{fig:multi_to_mono}). Dans tous les cas la réduction
nécessite de normaliser les objectifs et le décideur introduit un caractère préférentiel.
Dans le cas de la pondération, un coefficient est attribuer à chaque objectifs. Bien que
algorithmiquement relativement simple à mettre en place, le choix des coefficients peut
lui être complexe lorsque les divers objectifs ont des échelles ou unités différentes.
La méthode du compromis, considère un objectif et les autres doivent être formulés
sous forme de contraintes.
Dans ces deux approches, les méthodes ne permettent d’obtenir qu’une seule
solution à chaque itération.
Finalement la dernière méthode permet d’optimiser les différents objectifs en calculant
la distance de chaque solution par rapport à une solution de référence. Le choix
du point de référence est alors déterminant \parencite{Collette2002}, Fig. 2.10).
Cette dernière approche est donc fortement dépendante de la position du point de
référence et donc encore une fois de la connaissance a priori du problème.

Dans ces approches, l’introduction d’un caractère préférentiel lorsque la connaissance a priori
est limitée implique une recherche biaisé dont le risque est d’écarter des zones
de recherche qui pourraient être prometteuses. Ces méthodes ont cependant l’avantage
de réduire la complexité en transformant un problème multi-objectif en un problème
mono-objectif. Cette nouvelle formulation assure ainsi de trouver une unique solution
optimale au cours du processus d’optimisation.

\begin{figure}
    \begin{center}
        \includegraphics{Ressources/Images/optimisation/multi_to_mono.pdf}
    \end{center}
    \caption{Transformation d’un problème multi-objectif en optimisation mono-objectif.
             \label{fig:multi_to_mono}}
\end{figure}
% subsubsection decision_a_priori (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Décision interactive} % (fold)
\label{ssub:decision_interactive}
Dans cette approche le décideur oriente la recherche de manière itérative et le décideur
intervient de manière récurrente pour orienter la recherche
en fonction des résultats de l’optimisation. L’alternance entre optimisation,
analyse, et sélection de nouvelles contraintes permet de réduire l’espace de
recherche pour converger vers une solution répondant aux critères du décideur.
Cette méthode fait ainsi intervenir technicien et décideur de manière similaire
aux approches a posteriori à l’exception que le processus est itératif et progressif.


\iunsure{Ajouter description plus complète}
% subsubsection decision_interactive (end)


\subsubsection{Décision a posteriori} % (fold)
\label{ssub:decision_a_posteriori}
Contrairement aux autres approches, l’aide à la décision multi-objectif a posteriori
suppose qu’un ensemble de solution optimales ont été généré grâce à un processus
d’optimisation en amont. Lors de l’optimisation aucun caractère préférentiel
n’est introduit permettant d’explorer l’espace de décision sans restrictions.
De plus ce processus permet d’améliorer la compréhension du problème et ainsi
de simplifier la formulation des préférences sans nécessiter une expertise importante.
L’aide à la décision permet ainsi de trier, organiser ou encore sélectionner un
ensemble de solution dans l’espace de compromis formé par les solutions optimales.
Ces solutions étant toutes considérées comme optimales il est nécessaire d’introduire
un caractère subjectif à travers l’expérience et les préférences du décideur.
Plusieurs approches existent et peuvent principalement être classés en deux groupes,
les approches par critère de synthèse ou les approches par surclassement.



\paragraph{Approche par critère de synthèse :} % (fold)
\label{par:approche_par_critère_de_synthèse}
Dans cette approche l’ensemble des objectifs est réduit à une note unique permettant
de trier l’ensemble des solutions de l’espace de compromis.
L’approche est similaire aux méthodes d’aide à la décision a priori mais est dans
ce cas uniquement appliquée à l’ensemble de solutions optimales. Dans cette
configuration l’ordre est dit total car toute les solutions sont comparables
entre elles.

Une fonction unique dite d’utilité est formulée à partir de l’ensemble des critères
de décision. Pour chaque critère une fonction de valeur associée est construite en
fonction de l’appréciation par le décideur basé sur des éléments subjectifs.
L’ensemble de ces fonctions sont ensuite agrégées afin de construire la note unique.
De nombreuses techniques comme la somme, la moyenne, le produit, ou encore la distance
pondérée sont utilisées pour réaliser l’agrégation.
Les méthodes MAUT (Multiple Attribute Utility Theory) (\mtodo{Ajouter ref}{Fishburn 1970})
ou AHP (Analytic Hierarchy Process) (Thomas L. Saaty) sont deux exemples utilisant
l’approche par critère de synthèse.

\iunsure{Ajouter exemple d’application}

Ces approches comportent cependant un fort effet compensatoire car la formulation
de la note unique est réalisé par agrégation pondérée. Un critère ayant une valeur hautement
pénalisante peut ainsi être retenue si les autres critères ont en moyenne une valeur élevée.
% paragraph approche_par_critère_de_synthèse (end)


\paragraph{Approche par surclassement :} % (fold)
\label{par:approche_par_surclassement}
Contrairement à l’approche par critère de synthèse, la méthode de surclassement
permet de construire un ordre partiel entre les solutions. Il peut ainsi exister
des solutions non comparable après ce processus.
Afin de classer les solutions, une relation de surclassement est définie entre les
solutions optimales grâce à des éléments préférentiels.
Une solution $a$ surclasse une solution $b$ notée $aSb$ si $a$ est au moins aussi
bon que $b$ au regard des préférences du décideur.

Un ensemble de règle est alors défini pour chaque critère : préférence, indifférence, seuils, ...
Le surclassement est ensuite définie par un critère unique résultant de l’agrégation
des critères pondérés par des coefficients. Selon les méthodes on distingue trois
grands principes permettant d’éviter les effets compensatoires
\begin{itemize}
  \item Le principe de concordance stipule que la majorité des critères doivent
        vérifier la relation de surclassement.
  \item Le principe de non discordance stipule que les critères ne vérifiant pas
        la relation de surclassement ne doivent pas exprimer un désaccord trop
        important.
  \item Le principe de crédibilité qui vise à pondérer une hypothèse de surclassement
        en fonction de la pertinence du caractère préférentiel
\end{itemize}
Les solutions sont ainsi comparées deux à deux permettant de réduire le nombre de
solutions optimales dans le respect des préférences du décideur.
Les méthodes ELECTRE ou encore PROMETHEE font partis des nombreuses
méthodes développées (\mtodo{Ajouter ref}) et la sélection d’une approche est lié
au type de problématique. Par exemple dans une problématique de choix la méthode
ELECTRE I ou IS pourra être sélectionné. Dans une problématique de rangement les
méthodes ELECTRE II, III, ou IV pourront être utilisées.

\iunsure{Ajouter exemple d’application}
% paragraph approche_par_surclassement (end)
% subsection les_approches_existantes (end)


\subsection{Approche retenue} % (fold)
\label{sub:approche_retenue}
L’aide à la décision dans le secteur du bâtiment fait intervenir de nombreux acteurs
et le choix est alors le résultat d’un compromis. Afin de ne pas écarter prématurément
des solutions, une approche d’aide à la décision a posteriori a été retenue. L’ensemble
de solution optimales offre en effet une nouvelle connaissance sur le problème
facilitant l’introduction de préférences.
De plus il a été montré que la reformulation d’un problème multi-objectif sous la
forme d’un objectif unique introduit un biais impactant directement la qualité de
la solution finale (\mtodo{Ajouter Citation}{Blondeau}). L’approche a posteriori
apporte ainsi plus de souplesse dans l’aide à la décision et permet une meilleure
exploration de l’espace de décision (ensemble des.

Une fois la surface de compromis atteinte, l’introduction de la préférence de
du décideur permet de réduire le nombre de solution et/ou de les classer.
Dans ces travaux, une méthode par paramétrisation a été retenue pour sa simplicité
et pour son caractère interactif adapter à un nombre limitée de critère. Il est
ainsi possible graphiquement de sélectionner la solution adaptée aux préférences
du projet et le décideur est acteur de ces choix.

Dans l’optique d’une aide à la décision a posteriori, il est ainsi nécessaire
dans un premier temps de sélectionner et de réaliser une optimisation multi-objectif.
La section suivante introduit le concept d’optimisation multi-objectif ainsi que
les différentes approches existantes.

\itodo{Expliquer la raison du choix d’une approche a posteriori}
% subsection approche_retenue (end)
% section les_methodes_d_aide_à_la_decision (end)



% ..............................................................................
% ..............................................................................
\section{L’optimisation multi-objectif} % (fold)
\label{sec:l_optimisation_multi_objectif}
% ------------------------------------------------------------------------------
\subsection{Vocabulaire et Définition} % (fold)
\label{sub:vocabulaire_et_definition}
Dans le domaine de la thermique du bâtiment la caractérisation
de la performance est souvent effectuée de manière itérative à partir d’une solution
de référence ou bien de manière empirique dans l’espace de décision. La direction et
les variations évaluées résultant dans la plupart des cas de l’expérience du ou des
décideur[s]. La recherche est alors fortement biaisé en amont limitant l’exploration
d’alternatives et l’obtention d’une solution optimale est alors peu probable.
Dans un premier temps le vocabulaire nécessaire est introduit et une description
succincte des méthodes existantes est discutée.

Un problème d’optimisation multi-objectif (Définition\ref{def:optimisation_multi_objectif})
est défini par un espace de recherche et de solution multi-dimensionnel contrairement à
une approche mono-objectif où seul l’espace de recherche est multi-dimensionnel.
Afin de pouvoir classer et sélectionner les solutions entre elles, il est alors nécessaire de définir un
nouveau opérateur de classement : la relation de dominance.
L’approche la plus répandue ne considère pas de caractère préférentiel et repose sur
le principe de dominance au sens de Pareto (Définition\ref{def:dominance_de_pareto}.
Cette approche est généralisée à travers la définition du cône de probabilité où
un paramètre $\lambda$ est introduit. Sa variation permet de réduire ou agrandir
la surface couverte par la dominance et on retrouve la dominance de Pareto pour $\lambda = 0$.
Parmi les méthodes existantes, certaines comme l’approche lexicographique
introduisent une préférence tout en conservant la nature multi-objectif du problème (\munsure{Ajouter citation}).
Dans cette approche les objectifs sont classés selon un ordre d’importance et la comparaison est
faite dans le respect de cet ordre.
Afin d’obtenir plus d’informations, le lecteur intéressé est invité à consulter \cite{Collette2002}
pour une explication plus exhaustive et détaillée.

\begin{Def}[Optimisation multi-objectif]\label{def:optimisation_multi_objectif}
L’optimisation multi-objectif est le processus visant à minimiser ou maximiser un ensemble
d’objectifs tout en respectant un nombre fini de contraintes.
Il peut être formulé de la manière suivante dans le cas d’une minimisation :
\begin{equation}\label{eq:def_optimisation}
  \begin{aligned}
                           & \underset{\vec{x} \in \mathbb{R}^{n}}{\min(\vec{f}(\vec{x}))}& & \quad (f_{m})_{m \in [1 ... M]} & \longmapsto \mathbb{R} \\
    \text{Sujet à : }\quad & \vec{g}(\vec{x}) \leqslant 0                                 & & \quad (g_{j})_{j \in [1 ... J]} & \longmapsto \mathbb{R} \\
                           & \vec{h}(\vec{x}) = 0                                         & & \quad (g_{k})_{k \in [1 ... K]} & \longmapsto \mathbb{R} \\
  \end{aligned}
\end{equation}
Avec $\vec{x}$ représentant la valeur des $n$ variables de décisions et $\vec{f}(\vec{x})$
l’ensemble des $M$ fonctions objectif.  $\vec{g}(\vec{x})$ et $\vec{h}(\vec{x})$ représente
respectivement les $J$ contraintes d’inégalité et $K$ contraintes d’égalité imposées au problème
d’optimisation. L’optimisation multi-objectif est donc le processus qui cherche à améliorer la
qualité des solutions en faisant varier la valeur des variables de décisions ($\vec{x}$).
Lorsque $J > 0$ ou $K > 0$ on parle de problème d’optimisation sous contraintes.
\end{Def}

\begin{Def}[Dominance au sens de Pareto]\label{def:dominance_de_pareto}
On considère qu’un vecteur de décision, $\vec{x}_{a}$ domine un autre $\vec{x}_{b}$ si :
\begin{itemize}
  \item $\vec{x}_{a}$ est aussi bon que $\vec{x}_{b}$ sur tous les objectifs
  \item $\vec{x}_{a}$ est meilleur que $\vec{x}_{b}$ sur au moins un objectif
\end{itemize}
Cette relation sera notée sera notée : $\vec{x}_{a} \prec \vec{x}_{b}$.
Un point est ainsi dit optimal au sens de Pareto lorsque aucun autre point dans
l’ensemble le comprenant ne le domine ().
\end{Def} \mtodo{ajouter référence figure}

\ftodo{Ajouter figure montrant la relation de dominance. Partie dominée, dominante, neutre}

Dans ces travaux la relation de dominance au sens de Pareto a été retenue car
elle n’introduit pas de préférence a priori. La surface de compromis sera ainsi
décrite comme équivalent au front de Pareto (Définition\ref{def:front_de_pareto}).


\begin{Def}[Front de Pareto~:~PF]\label{def:front_de_pareto}
Il représente l’ensemble formé par les solutions non-dominées (espace des objectifs)
dont les vecteurs de décisions sont non-dominés.
Le front de Pareto est donc la représentation dans l’espace des objectifs
de l’ensemble optimal de Pareto qui lui est définie dans l’espace de décision.
Cette relation sera notée :
\begin{equation}
  \begin{aligned}
    PF   =& \left\{ \vec{f}(\vec{x}_{*}), \  \vec{x}_{*} \in POS \right\} \\
    POS  =& \left\{ \vec{x}_{*} \in \mathbb{R}^{n} \mid (\nexists \vec{x} \in \mathbb{R}^{n}) \  \vec{f}(\vec{x}) \prec \vec{f}(\vec{x}_{*}) \right\} \\
  \end{aligned}
\end{equation}
avec $POS$ l’ensemble optimal de Pareto (Pareto Optimal Set) et $PF$ le front de Pareto
(Pareto Front).
\end{Def}


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Points de référence} % (fold)
\label{ssub:points_de_reference}
Afin de guider la convergence, certaines méthodes utilise la position singulière
de points dans l’espace des objectifs.
\paragraph{Le point idéal :} % (fold)
\label{par:le_point_idéal}
Point (\mtodo{ajouter référence figure}) dans l’espace des objectifs dont
les coordonnées correspondent à l’optimal de chaque objectif pris séparément. Ce point
peut être utilisé pour guider la recherche. Cependant dans la plupart des cas, ce point
n’est pas une solution du problème multi-objectif dont les objectifs le composant
sont le plus souvent antinomiques.
% paragraph le_point_idéal (end)

\paragraph{Le point Nadir :} % (fold)
\label{par:le_point_nadir}
Point (\mtodo{ajouter référence figure}) ayant pour coordonnées la
valeur de la borne supérieure (cas d’une minimisation) pour chaque objectif du front
de Pareto. Si ce point est connu, il peut être utilisé pour restreindre l’espace
de recherche ou évaluer la qualité des solutions trouvées.
% paragraph le_point_nadir (end)

\ftodo{Ajouter graphique avec point idéal et point nadir}
% subsubsection points_de_reference (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{La convexité} % (fold)
\label{ssub:la_convexite}
Ce terme permet de caractériser la forme d’un ensemble.
Il est dit \emph{convexe} lorsque pour deux points distincts, la droite les reliant
est contenu dans cet ensemble \parencite{Collette2002}. Ce terme est utilisé pour décrire
la forme du front de Pareto dans le cas de problèmes multi-objectif. Dans le cas
de front continue, on parle alors de front convexe ou concave (\mtodo{ajouter ref image}).

\ftodo{Ajouter image d’un front convexe vs un front concave}
% subsubsection la_convexite (end)
% subsection vocabulaire_et_definition (end)



% ------------------------------------------------------------------------------
\subsection{Les méthodes exactes} % (fold)
\label{sub:les_methodes_exactes}
Ces méthodes regroupent l’ensemble des sous-méthodes permettant de caractériser
l’ensemble des solutions (Fig~\ref{fig:multi_exactes}). Elles sont cependant
coûteuses car un nombre très important de simulations est requis.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Algorithmes de chemin optimal} % (fold)
\label{ssub:algorithmes_de_chemin_optimal}
Ces approches permettent de trouver un chemin optimal à l’intérieur d’un problème
représenté à l’aide de graphes. Contrairement aux approches exhaustives, l’ensemble
des solutions n’est pas nécessairement évalué comme par exemple avec l’algorithme
A*. Cependant ces approches nécessitent la formulation du problème sous la forme d’un
graphe ainsi que la connaissance de la solution à atteindre.
% subsubsection algorithmes_de_chemin_optimal (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Programmation dynamique} % (fold)
\label{ssub:programmation_dynamique}
Cette approche se base sur la résolution de sous-problème pour résoudre le problème global
et se représente aussi sous forme d’un graphe. La recherche à l’intérieur du graphe est basée
sur le théorème de Bellmann qui stipule qu’un chemin optimal ne peux être composé
que de sous-chemin optimaux. Cette approche permet ainsi de garantir l’obtention
de toutes les solutions optimales tout en réduisant le nombre d’évaluations nécessaires.
Cette approche est particulièrement adaptée à l’optimisation de processus séquentiels
et \cite{Rivallain2013} l’utilise pour optimiser l’étude de cas de la barre Grimaud
et compare les solutions résultantes à une approche approchée. Ces approches sont
discutées dans la section suivante.
% subsubsection programmation_dynamique (end)

\begin{figure}
    \begin{center}
        \includegraphics{Ressources/Images/optimisation/exactes.pdf}
    \end{center}
    \caption{Transformation d’un problème multi-objectif en optimisation par méthodes
             exactes.
             \label{fig:multi_exactes}}
\end{figure}
% subsection les_methodes_exactes (end)



% ------------------------------------------------------------------------------
\subsection{Les méthodes approchées} % (fold)
\label{sub:les_methodes_approchees}
Ces approches contrairement aux approches exactes ne garantissent pas l’obtention
de l’ensemble du front de Pareto mais le nombre d’évaluation nécessaire est fortement
réduit. On distingue deux familles, les heuristiques, et les méta-heuristiques.
Un heuristique est spécifique au problème que l’on cherche à résoudre et est
inspiré de l’expérience ou des contraintes du problème. Il représente ainsi une
solution souvent sous-optimale ou non réalisable et est défini empiriquement permettant
de guider la recherche vers la solution optimale. L’utilisation d’une heuristique permet
ainsi d’accélérer la recherche mais il peut être délicat de le définir correctement pour
des problèmes complexes tout en garantissant la convergence vers la ou les solutions
exactes. Un heuristique qui accélère la recherche tout en garantissant de trouver
la solution exacte est alors dit admissible.

Considérons le problème suivant : trouver la distance minimale pour aller d’un point
A à un point B en considérant plusieurs obstacles (\mtodo{Ajouter ref figure}).
La position de départ et d’arrivée étant connues, ce problème peut être résolue
de manière exacte avec l’algorithme A* (\ref{ssub:algorithmes_de_chemin_optimal})
si on définit un heuristique admissible.

Afin de résoudre ce problème, la connaissance de la solution pour un cas plus simple,
le distance entre A et B sera utilisée : c’est l’heuristique (\mtodo{Ajouter ref figure}).
Dans le cas où les mouvements en diagonal sont admissibles la distance entre ces
deux point sera la distance euclidienne. Si seul les mouvements verticaux et
horizontaux sont admissibles alors la distance minimale est la distance de Manhattan.
La recherche favorisera ainsi dans un premier temps les mouvements réduisant cette
distance afin de concentrer la recherche vers les chemins prometteurs.

\ftodo{Ajouter illustration distances et recherche A*}

Comme nous venons de le voir un heuristique est fortement lié au problème et doit tenir
compte de ces contraintes propres. Cependant dans l’optimisation multi-objectif combinatoire
appliqué au bâtiment cette connaissance et souvent trop limité et la définition d’un
heuristique risque d’amener un biais trop important.

Dans l’optique d’une formulation plus générale, des méta-heuristiques ont ainsi été développés.
Ces formulations utilise un processus stochastique couplé à des éléments d’apprentissage
afin de guider la recherche et éviter l’évaluation complète des solutions (voir \ref{sub:les_methodes_exactes}).
% subsection les_methodes_approchees (end)



% ------------------------------------------------------------------------------
\subsection{Choix retenue} % (fold)
\label{sub:choix_retenue}
Dans cette section les différentes approches existantes ont été décrites.
La forte cardinalité du problème combinatoire et le temps important nécessaire
à l’évaluation d’une solution écarte les méthodes exactes qui sont trop coûteuses. Une
méthode approchée par méta-heuristique est alors retenue (Fig~\ref{fig:multi_meta}).
Même si ces approches ne garantissent pas l’obtention de l’ensemble des solutions
optimales au sens de Pareto, les travaux précédents ont montré (\munsure{Ajouter citation})
que ces approches permettent de l’approximer.

\begin{figure}
    \begin{center}
        \includegraphics{Ressources/Images/optimisation/meta_heuristique.pdf}
    \end{center}
    \caption{Transformation d’un problème multi-objectif en optimisation
             multi-objectif en utilisant un méta-heuristique.
             \label{fig:multi_meta}}
\end{figure}
% subsection choix_retenue (end)
% section l_optimisation_multi_objectif (end)




% ..............................................................................
% ..............................................................................
\section{Les Méta-heuristiques} % (fold)
\label{sec:les_meta_heuristiques}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Description} % (fold)
\label{sub:description}
Les méta-heuristiques sont des approches générales utilisant des éléments d’apprentissage
couplés à une recherche stochastique permettant d’explorer un domaine de recherche
en combinant exploration (Définition\ref{def:exploration}) et exploitation (Définition\ref{def:exploitation}).
C’est un processus itératif qui tient compte de la mémoire et de l’expérience acquise
à travers son évolution pour converger (Définition\ref{def:convergence})
vers l’ensemble formé par les solutions optimales (Définition\ref{def:optimum}).
Il existe de nombreux algorithmes dans la littérature et ils peuvent tous être classés
dans la branche de l’Intelligence Artificielle (IA).
La notion a été définie par John McCarthy et Marvin Lee Minsky (\mtodo{Ajouter ref})
et ne cesse de s’améliorer dans de nombreux domaines comme le déplacement organisé
en groupe (humains, oiseaux, poissons, ...) ou encore l’apprentissage et la réflexion.
Deep Blue a ainsi été le premier programme informatique à battre le champion du monde
d’échec de l’époque \parencite{Hsu199970}.
Le jeu de Go est un autre exemple témoignant de la forte évolution de l’IA. Alors que
les anciens programmes étaient loin d’être compétitifs contre le bas du classement
des joueurs professionnels, AlphaGo \parencite{Silver2016484} a battu le champion Européen (Fan Hui)
puis un des meilleurs joueur au monde au titre de 9 dan professionnel (Lee Sedol).

\begin{Def}[Exploration]\label{def:exploration}
Caractérise la capacité d’un méta-heuristique à explorer le domaine formé par les
variables de décision. Plus l’exploration est forte plus grand est l’espace couvert
et plus grandes sont les chances de trouver les solutions optimales.
\end{Def}

\begin{Def}[Exploitation]\label{def:exploitation}
Caractérise la capacité d’un méta-heuristique à améliorer les solutions existantes
afin d’orienter la recherche vers les solutions optimales. Plus l’exploitation est
grande plus la convergence de l’algorithme est rapide.
\end{Def}

\begin{Def}[Convergence]\label{def:convergence}
Dans le cas des méthodes approchées la convergence est caractériser par l’arrêt
de l’amélioration des solutions existantes. Les méta-heuristiques étant des
méthodes approchées, cette stagnation ne signifie pas que le front de Pareto réel
a été atteint, seulement que les solutions ne progressent plus. Il est ainsi
nécessaire de faire la distinction entre les optimums dits locaux et ceux dits
globaux (Définition\ref{def:optimum}.
\end{Def}

\begin{Def}[Optimum locaux et globaux]\label{def:optimum}
Il existe deux types d’optimum : les locaux (fort ou faible) et les globaux.
Un optimum est considéré comme local fort quand il domine l’ensemble des solutions
du \emph{voisinage} et faible si il est seulement non-dominé dans ce voisinage (plusieurs
optimum locaux identiques existent). Un optimum est considéré comme global lorsque il domine
l’ensemble des solutions existantes dans le domaine de décision. Dans le cas d’une
optimisation multi-objectif, l’ensemble des solutions du front de Pareto sont des
optimum globaux formant la surface de compromis dans le respect de l’espace de
décision.
\end{Def}

\paragraph{} % (fold)
La performance d’un méta-heuristique réside dans sa capacité à faire
un bon compromis entre exploitation et exploration afin d’augmenter les chances de
converger vers le front de Pareto formé par les optimums globaux.
En effet, améliorer l’exploration permet d’éviter de converger vers des optimums
locaux mais ralenti la vitesse de convergence. À l’inverse si l’exploitation est
trop forte, la diversité des solutions va diminuer et entraîner la stagnation de
l’algorithme sur des front dits locaux. Ce phénomène est particulièrement
vrai pour des problèmes multi-modaux et ceux ayant des optimums dits trompeurs.
Un problème est dit multi-modal lorsque il considère de nombreux optimums locaux
augmentant les chances pour l’algorithme de rester bloquer. Enfin un problème est
dit trompeur lorsque la majorité de l’espace de décision favorise un front local
et qu’une faible partie de l’espace permet d’atteindre le front de Pareto.
% paragraph  (end)
% subsection description (end)


% ------------------------------------------------------------------------------
\subsection{Les méthodes existantes} % (fold)
\label{sub:les_methodes_existantes}
La plupart des méta-heuristiques s’inspirent du vivant et des différents principes
le décrivant. On distingue deux grande familles : les méta-heuristiques à population
et les méta-heuristiques de voisinage (Fig~\ref{fig:multi_meta_detail}). Parmi
les approches par voisinage les plus connus sont le recuit simulé et la recherche tabou.
Ces approches cherchent à optimiser une solution grâce à une recherche locale combiné
à une stratégie améliorant l’exploration afin d’éviter de converger vers des optimums locaux.
Parmi les approches par population, deux grandes familles se distinguent: les algorithmes
évolutionnaires et les algorithmes d’essaim.

\begin{figure}
    \begin{center}
        \includegraphics{Ressources/Images/optimisation/meta_heuristique_detail.pdf}
    \end{center}
    \caption{L’optimisation multi-objectif par méta-heuristique.
             \label{fig:multi_meta_detail}}
\end{figure}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Les algorithmes évolutionnaires} % (fold)
\label{ssub:les_algorithmes_evolutionnaires}
Ces méthodes sont basées sur les mécanismes de la sélection naturelle. L’approche la plus
répandue et l’utilisation de l’algorithme NSGA II \parencite{Deb2002182} mais de nombreuses
variations existent. L’algorithme SPEA2 par exemple améliore la diversité du front de Pareto
grâce à une approche par clustering au prix d’une convergence plus lente \parencite{Zitzler2001}.
À l’inverse l’algorithme PESA lui tant à améliorer la convergence au prix de la diversité des
solutions du front final. Appliqué au bâtiment, \cite{Rivallain2013}
a utilisé une méthode approchée (NSGA-II) pour identifier des programmes séquentiels
efficaces de réhabilitation énergétique. Il a ainsi optimisé la combinaison des
modifications pour chaque phase mais aussi l’ordre dans lequel ces améliorations
doivent être réalisées. Pour les différentes solutions l’impact environnemental,
le confort des occupants en période estivale, et le coût ont été évaluées.
Plus récemment, (\mtodo{Ajouter Reich}) a utilisé l’algorithme NSGA II pour
optimiser de manière combiné le coût, l’émission de Gaz à effet de serre et
les besoins d’une maison à énergie positive.
Dans les deux cas, les résultats montrent que l’approche par méta-heuristique permet
de récupérer la majorité des solutions Pareto optimales pour un nombre
fortement réduit d’évaluation.
% subsubsection les_algorithmes_evolutionnaires (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Les algorithmes d’essaim} % (fold)
\label{ssub:les_algorithmes_d_essaim}
Ces méthodes sont basées sur l’expérience du vivant et plus particulièrement de
l’organisation au sein d’un groupe, d’un essaim, ou d’une colonie. Parmi les plus connus,
l’algorithme de colonie de fourmis a été proposé à l’origine par \cite{Colorni1992509}.
Comme pour les algorithmes évolutionnaires, de nombreuses variations ont été proposées
permettant d’étendre l’application aux problèmes multi-objectif et combinatoires
\parencite{MichaelGuntsch2003,Shea2006627}.
La méthode s’inspire du comportement des colonies de fourmis où chaque fourmis
explore une partie du domaine de décision en laissant une trace de phéromone derrière lui.
Le niveau de phéromone permet alors de guider les autres fourmis vers les solutions
optimales et son évaporation évite la stagnation imposant un équilibre entre exploration
et exploitation.
Le PSO (Particule Swarm Optimization), un autre algorithme, s’inspire du comportement des oiseaux
qui volent de manière synchrone et qui sont capables d’éviter les collisions même lors
de changements brusques de direction. Cette approche a été appliquée au domaine
du bâtiment pour l’optimisation multi-objectif en tenant compte du confort des occupants,
de l’impact environnemental, des besoins en énergie, et de la sécurité de l’ouvrage \parencite{Armand-Decker2015}.
Il existe de nombreuses autres implémentations et le lecteur intéressé pourra consulter
\cite{Aboul-EllaHassanien2015} qui décrit les approches récentes et leur applications.
% subsubsection les_algorithmes_d_essaim (end)
% subsection les_methodes_existantes (end)


% ------------------------------------------------------------------------------
\subsection{Comment choisir ?} % (fold)
\label{sub:comment_choisir}
De nombreuses approches ont été développées mais il n’existe cependant pas de méthode
permettant de sélectionner la plus adaptée : \cite{Wolpert199767} formalise
ce concept. Le choix d’une approche est alors subjectif ou dicté par la complexité
de mise en œuvre de l’approche.
Cependant, tous les méta-heuristiques ont en commun l’introduction de
mécanisme stochastiques et nécessitent le réglage de différents paramètres.
Ces paramètres sont dans la majorité des études déterminés de manière empirique
même si certaines méthodes existent pour automatiser ce paramétrage. \cite{Lopez-Ibanez2012861}
a ainsi adapté l’algorithme F-Race \parencite{Birattari2010311} aux problèmes multi-objectifs
et propose un outil permettant d’automatiser la paramétrisation de différent algorithmes de colonies
de fourmis \parencite{Lopez-Ibanez2012861}.
Les auteurs montrent ainsi qu’un réglage automatique par apprentissage permet d’améliorer
la performance des algorithmes par rapport à un réglage empirique.
D’un algorithme à un autre, l’importance tout comme le nombre de paramètres varie
fortement. Il apparaît alors important de considérer une approche dont le nombre
de paramètre est réduit et dont le paramétrage est intuitif.
Un autre facteur à prendre en compte est la nature du problème. Certaines formulations
fortes rendent difficile leur adaptation à une optimisation dans le bâtiment.
Il est nécessaire que l’approche retenue soit assez polyvalente pour formuler des
problèmes mixtes combinant variables discrètes, continues, et qualitatives.
Finalement le couplage entre l’outil d’optimisation et l’outil utilisé pour décrire
le problème doit être la plus simple possible afin d’éviter les problèmes d’interfaçage.


{
\noindent
Le choix d’une approche peut ainsi être guidée par les réponses aux propositions suivantes :
\begin{itemize}
  \item Quelle est la nature des variables de décision (discrètes, continues, ...) ?
  \item Quelle sont les contraintes imposées par l’outil de modélisation ?
  \item La méthode requiert-elle de nombreux paramètres à définir ?
  \item La définition de ces paramètres est-elle ardue ?
    \begin{itemize}
      \item La sensibilité est forte ? Dur à paramétrer
      \item Chaque paramètre a un impact identifiable en amont ? Simple à paramétrer
      \item Quel nombre d’itération nécessaire pour régler ces paramètres ?
    \end{itemize}
  \item Une implémentation de la méthode existe ?
    \begin{itemize}
      \item Elle répond aux contraintes de l’outil de modélisation ?
      \item Elle est adapté à la nature des variables de décision ?
      \item La formulation du problème est-il adapté à la méthode ?
    \end{itemize}
  \item Quelle est la quantité de travail nécessaire pour l’implémenter ?
    \begin{itemize}
      \item Plus de travail que le couplage nécessaire entre les outils ?
    \end{itemize}
\end{itemize}
}
% subsection comment_choisir (end)
% section les_meta_heuristiques (end)




% % ..............................................................................
% % ..............................................................................
\section{Artificial Bee Colony (ABC)} % (fold)
\label{sec:artificial_bee_colony}
Notre problème étant de nature mixte (paramètres discrets, continues, et qualitatifs)
la formulation de l’algorithme doit être flexible. Les approches aux formulations
trop strictes comme les algorithmes de colonie de fourmis ne sont alors pas applicables.
De plus, au vu des temps de simulation importants, l’algorithme doit être robuste
et converger vers le front de Pareto sans nécessiter un paramétrage complexe.
En effet, ces paramètres sont sélectionnés de manière empirique et trouver la
bonne combinaison est un problème d’optimisation en lui-même. Un algorithme ayant
un nombre limité de paramètre limite ainsi le nombre d’itération nécessaire pour
les déterminer.

Contrairement aux approches les plus couramment utilisées (\mtodo{Ajouter lien vers tableau})
l’algorithme l’ABC requiert la définition de seulement 2
paramètres, faisant de lui un candidat adapté dont la formulation est intuitive.
De plus ces deux paramètres impact de manière prévisible les proportions d’exploration
et d’exploitation. Le premier, la taille de la population, permet d’augmenter
l’exploration mais une population trop importante risque de ralentir la convergence.
Le nombre d’essai maximum quand à lui permet de contrôler l’exploitation. Un nombre
trop faible entraîne un abandon prématuré d’une solution potentielle.
\ttodo{Ajouter comparatif du nombre de paramètres en fonction des approches
       (PSO, ABC, NSGA II, ...). }

Ensuite il est nécessaire que l’approche soit simple à coupler avec les outils
utilisés. La restriction principale étant le couplage avec le logiciel Dymola par
l’intermédiaire de son langage de script.
Comme il a été décrit dans le chapitre précédent, un outil a été développé sur la
base de la bibliothèque BuildingsPy afin de simuler de manière concurrente le modèle
et ainsi fortement réduire le temps nécessaire par simulation.
Afin de profiter de ces outils et simplifier le couplage, seule les solutions
développées en Python sont retenues. Les suites logiciels tels que
\href{http://jmetal.sourceforge.net/index.html}{jMetal} ou
\href{http://moeaframework.org/index.html}{MOEA Framework} sont ainsi écartées.

Dans un premier temps DEAP (\mtodo{Ajouter source}) une bibliothèque Python a été
choisi. Elle est construite de manière à permettre à l’utilisateur de se concentrer sur la
logique de l’algorithme en ajoutant une couche d’abstraction sur la gestion et la
création des structures nécessaires.
Bien que compatible avec les différents outils utilisés, la bibliothèque ne sera
pas retenue car les briques existantes sont principalement utiles pour créer un
algorithme évolutionnaires. Par exemple la gestion de la nature des variables,
les marches aléatoires, ou encore les différentes phases de l’algorithme ABC
doivent être implémentées.

L’approche retenue a ainsi été de développer une bibliothèque permettant une utilisation
simple et intuitive de l’algorithme dans l’optique de la ré-utilisabilité. De plus,
de nombreuses améliorations proposées dans la littérature ont ainsi été mis en place
afin de construire une méthode alternative aux approches plus traditionnelles.
La section suivante présente plus en détail l’histoire de l’ABC et l’évolution de
son fonctionnement à travers les améliorations sélectionnées.
Le but étant d’améliorer l’exploration et l’exploitation tout en limitant la quantité
de paramètres à définir de manière empirique afin de simplifier son utilisation.



% ------------------------------------------------------------------------------
\subsection{Inspiration} % (fold)
\label{sub:inspiration}
Comme de nombreux méta-heuristiques, l’ABC est inspiré du comportement des êtres
vivants, ici plus particulièrement, du comportement des abeilles mellifères. De
nombreux travaux ont vu le jour sur le comportement de ces abeilles comme leurs méthodes
de communication et le fonctionnement de la récolte. \cite{Visscher19821790} montre
que le choix de chaque abeille est guidé par l’ensemble des informations acquises
par l’essaim afin de constamment ajuster les sources utilisées pour la récolte.
Un modèle mathématique est décrit par \cite{Camazine1991547} permettant de simuler
ce comportement social caractéristique des abeilles.
Cependant, l’idée de s’inspirer des abeilles mellifères pour l’optimisation
n’est introduit que en 2005, et cette approche fait donc partie des plus récentes \parencite{Karaboga2005}.
De nombreuses approches ont depuis vu le jour (VirtualBee, Bee Colony Optimization
(BCO), ...) mais la formulation de l’ABC reste la plus répandue (\parencite{Karaboga201221})
et sera retenue dans ces travaux.

\iunsure{Ajouter figure explicitant le pourcentage d’utilisation des différentes approches}
% subsection inspiration (end)


% ------------------------------------------------------------------------------
\subsection{Principe de fonctionnement} % (fold)
\label{sub:principe_de_fonctionnement}
L’algorithme s’appuie sur les mécanismes d’organisation sociale des insectes \parencite{Bonabeau1999}
définit à travers quatre propriétés.

\paragraph{Le retour positif (Positive feedback) :} % (fold)
\label{par:positive_feedback}
\itodo{ICI}
Chaque individu partage ses connaissances avec le reste du groupe afin de guider
la recherche vers des sources de bonne qualité. Ce mécanisme fournit un indicateur
d’apprentissage pouvant être utilisé pour construire de nouvelles sources.
% paragraph positive_feedback (end)

\paragraph{Le retour négatif (Negative feedback) :} % (fold)
\label{par:negative_feedback}
Afin de prévenir les dangers ou d’éviter les sources de mauvaises qualités, un mécanisme est
aussi nécessaire. Celui-ci permet de contrebalancer le retour positif pour équilibrer
et améliorer le travail de groupe.
% paragraph negative_feedback (end)

\paragraph{Fluctuation :} % (fold)
\label{par:fluctuation}
Afin de pouvoir faire émerger de nouvelles solutions et remplacer celle qui se
tarissent, il est nécessaire d’ajouter un comportement stochastique à l’exploration
afin de prévenir la raréfaction des ressources. Ce comportement est observé
chez les abeilles avec les $Scout$s qui ne tiennent pas compte des retours positifs
et/ou négatifs et cherchent par eux-même de nouvelles sources. Ce mécanisme est ainsi
responsable de la diversité et du renouvellement.
% paragraph fluctuation (end)

\paragraph{Interactions :} % (fold)
\label{par:intractions}
Finalement afin de partager les retours négatifs et positifs un mécanisme de communication
est nécessaire.
Les abeilles transmettent les informations positives et négatives de la même
manière : grâce à la danse (danse en rond et en huit).
Ce mécanisme permet de transmettre de manière détaillé la qualité d’une source, sa
distance par rapport à l’essaim, et des dangers potentiels rencontrés (Fig~\ref{fig:bee_dance}).

\begin{figure}
    \begin{center}
        \includegraphics{abc/BeeDance.png}
    \end{center}
    \caption{Description du comportement social des abeilles.
             \label{fig:bee_dance}}
\end{figure}
\FloatBarrier
% paragraph intractions (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Formulation} % (fold)
\label{ssub:formulation}
L’algorithme ABC est une approche itérative constitué de 3 phases principales.
En amont de ce processus itératif, $N$ sources sont initialisées aléatoirement \eqref{eq:init_source}.
Chaque source $i$ est un vecteur $\vec{x}_{i}(i = 1, 2, \dotsc, N)$ de dimension $D$
avec $D$ le nombre de variable de décision définit dans l’espace de recherche.

\begin{equation}\label{eq:init_source}
  x_{ij} = x_{j}^{min} + RandUniform(0, 1) \times (x_{j}^{max} - x_{j}^{min}) \\
\end{equation}

\noindent Chaque phase est caractérisée par un type spécifique d’abeille :
\begin{itemize}
  \item Les \emph{butineuses} (Employed) : Elles sont responsables de l’exploration.
        Elles ramènent les informations recueillis à la ruche et réalisent une danse
        permettant au autres individus d’identifier la qualité et la direction des
        sources prometteuses.
  \item Les \emph{ouvrières} (Onlookers) : Elles assistent à la danse des butineuses
        et sélectionnent ensuite les sources prometteuses. La sélection est définie
        aléatoirement à partir d’une table de probabilité biaisé par la qualité des sources.
        Plus la source est prometteuse, plus la probabilité de la sélectionner est importante.
        Ces abeilles sont donc responsables de l’exploitation.
  \item Les \emph{Éclaireuses} (Scouts) : Elles explorent aléatoirement l’espace
        de décision et ramènent les informations à la ruche. Ce mécanisme
        permet de diversifier la recherche et d’éviter la stagnation. La quantité
        d’éclaireuses dans les ruches est estimé à 5/10\si{\percent} \parencite{Seeley1996}
\end{itemize}

L’Algorithme~\ref{alg:ABC_phases} décrit le principe de l’algorithme ABC original.
À chaque itération les sources sont mises à jour par les butineuses. Ensuite une
table de probabilité biaisée est créé : une probabilité est attribué à chaque source
en fonction de leur qualité. Ensuite, les ouvrières exploitent les sources sélectionnées
et la position des sources est mise à jour. Finalement les sources non fructueuses sont
remplacées par une nouvelle position aléatoire définit par les éclaireuses.
\begin{algorithm}\label{alg:ABC_phases}
  \SetAlgoVlined
  \emph{Initialisation des sources (population) selon \eqref{eq:init_source}}\;
  \While{Critère d’arrêt non atteint}
  {
  \For{$i \leftarrow 0$ \KwTo $N$}
  {
    \emph{Amélioration des sources par les butineuses grâce à une autre source $k$ \eqref{eq:update_source}}\;
    \emph{Attribution des probabilités à chaque source en fonction de leur qualité
          \eqref{eq:attribution_prob_to_source}}\;
    \emph{Amélioration des sources par les ouvrières grâce une autre source $k$ \eqref{eq:update_source}
          sélectionnée suivant la table de probabilité}\;
    \emph{Si la qualité de la source stagne réinitialiser sa position selon \eqref{eq:init_source}}\;
  }
  }
  \caption{Principe de l’algorithme ABC.}
\end{algorithm}

% Attribution des probabilités
\begin{subequations}\label{eq:attribution_prob_to_source}
  \begin{align}
    prob_{k} = &\frac{Qualite(\vec{x}_{k})}{\sum_{i=1}^{NbrSources} Qualite(\vec{x}_{i})} \\[1em]
    \shortintertext{avec}
    Qualite(\vec{x}_{k}) = &\frac{Dominance(k)}{NbrSources}
  \end{align}
  Où \emph{Dominance(k)} est le nombre de source que la source $k$ domine.
\end{subequations}

\begin{equation}\label{eq:update_source}
  \check{x_{ij}} = x_{ij} + RandUniform(-1, 1) \times (x_{ij} - x_{kj}) \\
\end{equation}
$x_{ij}$ étant la position d’une source $i$ ($i \in \{1, 2, \dotsc, N\}$) pour la
variable $j$ ($j \in \{1, 2, \dotsc, D\}$) et $x_{j}^{max}$ et $x_{j}^{min}$
étant respectivement ces bornes maximales et minimales.


Le lecteur souhaitant avoir plus d’information sur l’algorithme, son origine
et/ou son fonctionnement pourra lire : \cite{Karaboga201221,Aboul-EllaHassanien2015}.
% subsubsection formulation (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Extensions} % (fold)
\label{ssub:extensions}
À l’origine pensé pour résoudre des problèmes continues, il a été adapté aux problèmes
d’optimisation binaire \cite{Kashan2012342}, combinatoire \cite{Karaboga20113021}.
Ce méta-heuristique a ainsi été utilisé pour résoudre des problèmes de différent
domaine, dont l’entraînement de réseaux de neurones \parencite{Karaboga2007},
le génie électrique/mécanique/civil \parencite{Rao2009887}, ou encore le
clustering \parencite{Zhang20104761}. On retrouve aussi cet algorithme dans l’optimisation
de système de chauffage \parencite{Atashkari2011} ou dans des problèmes sous
contraintes \parencite{Tsai201480,Karaboga20113021}. Malgré son jeune âge, la littérature
sur les colonies d’abeilles augmente et continue de se diversifier pour mieux répondre
aux différents problèmes d’optimisations.

\ftodo{Ajouter un graphique montrant l’attrait pour ces techniques (nombre d’utilisation en cours des années) \cite{Karaboga201221}}

Le méta-heuristique étant plus faible en exploitation que en exploration, de nombreuses
propositions d’améliorations ont vu le jour. Certains s’inspirent des algorithmes
évolutionnaires \parencite{Bi2011174,Zhao2010558}. S’inspirant du PSO en ajoutant
la prise en compte de l’inertie et de la meilleure solution actuelle, \cite{Zhu20103166}
développe l’algorithme GBest Artificial Bee Colony (GABC).
\cite{Li2012320} mimique la formulation du PSO en ajoutant une inertie, l’accélération et la prise
en compte de la meilleure solution et le nomme Improved Artificial Bee Colony (I-ABC).
Afin d’améliorer la diversité dans la population initiale, \cite{Xiang20131256}
utilise la théorie du chaos pour générer la position initiale des sources en place
d’une initialisation stochastique. Il peut aussi être cité une modification du comportement
des exploratrices proposée par \cite{Sharma2012213} basée sur la distribution de
Lévy.

L’algorithme a aussi était adapté afin de résoudre des problèmes multi-objectifs.
Une approche comprenant un unique essaim est proposé par \cite{Akbari201239}
et le nomme MO-ABC pour Multi-Objective Artificial Bee Colony.
L’auteur utilise une archive basée sur l’$\epsilon$-dominance afin de maintenir
la diversité et assurer la convergence vers le front de Pareto. Durant les phases
des butineuses et des ouvrières seulement une variable est mis à jour en utilisant
une version modifiée de l’équation \eqref{eq:update_source} où $k$ représente une
solution de l’archive sélectionnée aléatoirement.
\cite{Omkar2011489} considère un essaim par objectif et nomme son approche : VEABC
pour Vector Evaluated Artificial Bee Colony.
Une collaboration est mise en place permettant aux sources d’un essaim d’être mis
à jour à partir des sources d’un autre essaim. L’approche est alors appliquée pour un
problème sous contraintes afin de minimiser le coût de fabrication (matériaux + production)
et le poids du composant tout en garantissant une résistance suffisante.
\cite{Zhang20121} développe aussi une approche basée sur plusieurs essaims mais ici,
chaque essaim considère l’ensemble des objectifs. En utilisant les distances de Hamming
et de Crowding, l’auteur ajoute une coopération entre les essaims pour la sélection
des sources de référence. L’auteur intègre de plus une méthode de pénalité adaptative
\parencite{Woldesenbet20073077} afin de pouvoir traiter des problème sous contraintes.

La sous-section suivante présente les améliorations retenues dans ces travaux.
Le but étant d’améliorer l’exploration et l’exploitation tout en limitant la
quantité des paramètres à définir de manière empirique afin de conserver un
méta-heuristique simple d’utilisation.
% subsubsection extensions (end)
% subsection principe_de_fonctionnement (end)



% ------------------------------------------------------------------------------
\subsection{Vol de Lévy} % (fold)
\label{sub:vol_de_levy}
Il a été observé chez diverses espèces comme les atèles (singes), les albatros,
ou encore la famille des Tephritidae (petites mouches) un comportement respectant
une marche aléatoire (Définition\ref{def:marche_aleatoire}) connu sous le nom de
Vol de Lévy (Définition\ref{def:vol_levy}).
Les marches aléatoires sont souvent utilisés dans les méta-heuristiques afin de
modifier les solutions déjà existantes dans l’espace de décision tout en tenant
compte d’un indicateur d’apprentissage. Par exemple, une loi normale permet
de renforcer une solution localement alors qu’une loi uniforme permet d’explorer
de manière analogue le domaine. Le lecteur pourra noter la forme caractéristique
de la distribution de Lévy retenue par rapport aux approches courantes à travers
la Fig.~\ref{fig:distribution_pdf}.

Dans \cite{Sharma2012213}, le vol de Lévy est utilisé afin de réaliser une recherche locale
et \cite{Hakli2013254} l’utilise pour améliorer l’exploitation durant la phase des exploratrices.
Le vol de Lévy est aussi utilisé dans un méta-heuristique récent: le Cuckko search.
Inspiré du comportement parasitaire de la reproduction des cuculidés, il a aussi fait l’objet
d’une adaptation aux problèmes multi-critères \parencite{Yang20131616}.

\begin{Def}[Marche aléatoire]\label{def:marche_aleatoire}
Une marche aléatoire \parencite{Yang201445} peut être définie comme une succession de pas
aléatoire dont chaque état ne dépend que de l’état précédent. Si on note $S_{N}$
la somme des pas aléatoires consécutifs $X_{i}$ alors $S_{N}$ est une marche aléatoire:
\begin{equation}\label{eq:marche_aleatoire}
    \begin{split}
        S_{N} &= \sum_{i=1}^{N} X_{i} = X_{1} + ... + X_{N}\\
              &= S_{N-1} + X_{N}
    \end{split}
\end{equation}
\end{Def}

\begin{figure}
    \begin{center}
        \includegraphics{LevyFlight/distribution_pdf.png}
    \end{center}
    \caption{Densité de probabilité des lois de cauchy, uniforme, normale, Lévy stable et symétrique.
             L’expression analytique est représentée par la courbe noire.
             \label{fig:distribution_pdf}}
\end{figure}

\begin{Def}[Vol de Lévy]\label{def:vol_levy}
Un vol de Lévy est une marche aléatoire dont la longueur du pas (positif ou négatif)
est définie aléatoirement à partir d’une distribution de Lévy stable et symétrique
couramment décrite par la transformée de Fourier suivante:
\begin{equation}\label{eq:fourier_levy}
    \mathcal{F}(k) = \exp(-\alpha\mathopen{|}k\mathclose{|}^{\beta}) \qquad  0 < \beta \leq 2
\end{equation}
\begin{equation}\label{eq:dist_levy}
    L(s) = \frac{1}{\pi} \int_{0}^{\infty} \cos(k s)\exp(-\alpha k^{\beta}) dk \qquad  0 < \beta \leq 2,
           \quad \alpha > 0
\end{equation}
On peut alors définir un Vol de Lévy pour une position initiale $x$ comme :
\begin{equation}
  x(t + 1) = x(t) + \alpha \times longueurPas \times \Delta
\end{equation}
avec $\alpha$ un facteur d’échelle permettant de limiter l’agressivité du pas,
$longueurPas$ un nombre tiré aléatoirement dans la distribution de Lévy et $\Delta$
un indicateur d’apprentissage propre au méta-heuristique.
\end{Def}

Sa forme analytique \eqref{eq:dist_levy} est uniquement connue pour $\beta = 2$
(distribution gaussienne) et $\beta = 1$ (distribution de Cauchy).
L’algorithme de Mantegna \parencite{Mantegna19944677} est alors utilisé afin
d’approximer cette distribution pour $0.3 < \beta < 1.99$ \eqref{eq:mantegna_algo}.

% \begin{subequations}\label{eq:mantegna_algo}
%   \begin{align}
%     s &= \frac{u}{\mathopen{|}v\mathclose{|}^{\nicefrac{1}{\beta}}}, \qquad u \sim \mathcal{N}(0, \sigma_{u}^{2}),
%         \quad v \sim \mathcal{N}(0, \sigma_{v}^{2}) \\[1em]
%     \shortintertext{avec}
%     \sigma_{u} &= \left[ \frac{\mathbf{\Gamma}(1+\beta)\sin(\pi\frac{\beta}{2})}%
%                              {\mathbf{\Gamma} \left[\frac{(1+\beta)}{2}\beta 2^{\frac{(\beta-1)}{2}}\right]}\right]^{\nicefrac{1}{\beta}},%
%     \qquad \sigma_{v} = 1 \\[1em]
%     \shortintertext{Où $\mathbf{\Gamma}$ représentant la fonction Gamma qui est l’extension analytique de la fonction factoriel pour
%                     les complexes et les réels ($\mathbf{\Gamma}(\beta) = (\beta -1)!, \quad \beta\in \mathbb{N}$) et est définie par:}
%     \mathbf{\Gamma}(\beta) &= \int_{0}^{\infty} t^{\beta-1}e^{-t} dt
%   \end{align}
% \end{subequations}


\begin{align}\label{eq:mantegna_algo}
    s &= \frac{u}{\mathopen{|}v\mathclose{|}^{\nicefrac{1}{\beta}}}, \qquad u \sim \mathcal{N}(0, \sigma_{u}^{2}),
        \quad v \sim \mathcal{N}(0, \sigma_{v}^{2}) \\[1em]
    \shortintertext{avec}
    \sigma_{u} &= \left[ \frac{\mathbf{\Gamma}(1+\beta)\sin(\pi\frac{\beta}{2})}%
                             {\mathbf{\Gamma} \left[\frac{(1+\beta)}{2}\beta 2^{\frac{(\beta-1)}{2}}\right]}\right]^{\nicefrac{1}{\beta}},%
    \qquad \sigma_{v} = 1 \notag
\end{align}
Où $\mathbf{\Gamma}$ représentant la fonction Gamma qui est l’extension analytique de la
fonction factoriel pour les complexes et les réels ($\mathbf{\Gamma}(\beta) = (\beta -1)!,\quad \beta\in \mathbb{N}$)
et est définie par:
\begin{equation*}
  \mathbf{\Gamma}(\beta) = \int_{0}^{\infty} t^{\beta-1}e^{-t} dt
\end{equation*}

Le vol de Lévy peut ainsi être assimilé à une intensification (forte probabilité de générer un petit pas)
couplé à de l’exploration (faible probabilité de générer un saut important) décrit par
l’effet d’île caractéristique du vol de Lévy (Fig.~\ref{fig:levy_vs_gaussian})
Il est ainsi possible d’améliorer l’exploration mais aussi de se dégager des optimums
locaux ou des solutions non prometteuses. Afin d’améliorer l’exploitation et l’exploration
ces travaux implémente un vol de Lévy comme marche aléatoire durant la phase des butineuses.

\begin{figure}
    \begin{center}
        \includegraphics{LevyFlight/levy_vs_gaussian.pdf}
    \end{center}
    \caption{Mouvement brownien (en vert) et vol de Lévy (en bleu) pour 200 pas aléatoires.
             \label{fig:levy_vs_gaussian}}
\end{figure}
\FloatBarrier
% subsection vol_de_levy (end)


% ------------------------------------------------------------------------------
\subsection{Apprentissage par vecteur opposé} % (fold)
\label{sub:apprentissage_par_vecteur_oppose}
Les méthodes d’optimisation et spécifiquement les méthodes approchées sont fortement
dépendantes de la population initiale (\mtodo{Ajouter source}). Il est ainsi nécessaire
de constituer une population couvrant de manière homogène l’espace de décision.
L’approche la plus répandu est d’initialiser la population initiale stochastiquement
à l’aide d’une distribution uniforme suivant \eqref{alg:init_phase}.
Afin d’améliorer la diversité / qualité de la population initiale sans connaissances
a priori,\cite{Tizhoosh2005695} a développé l’apprentissage par vecteur opposé
(OBL pour Opposite Based Learning) (Définition\ref{def:oblm}).

\begin{Def}[OBL~:~Apprentissage par vecteur opposé]\label{def:oblm}
Admettons un position de dimension $D$, $\vec{x}(x_{1}, x_{2}, ..., x_{D})$ avec
$x_{1}, x_{2}, ..., x_{D}$ des valeurs bornées. Si $x_{i} \in [a_{i}, b_{i}]$ pour
$i = 1, 2, ..., D$ alors la position opposée est $\vec{\check{x}}(\check{x_{1}},%
\check{x_{2}}, ..., \check{x_{D}})$ est définie par:
\[\check{x_{i}} = a_{i} + b_{i} - x_{i}\]
\end{Def}

\cite{Rahnamayan2008906} l’adapte pour l’algorithme Differential Evolution (DE)
et montre que la probabilité d’améliorer une solution est plus grande si on sélectionne
la position opposée que si on tire aléatoirement une autre position.
Dans son implémentation, il l’utilise pour l’initialisation mais aussi au cours du
processus d’optimisation. Les bornes ($a_{i}$ et $b_{i}$) sont alors définies dynamiquement
afin d’éviter de ralentir la convergence.
Cette approche a aussi été appliquée avec succès dans le cas de l’algorithme ABC
en couplage avec un opérateur de mutation \parencite{Bi2011174} ou encore en coopération
avec une marche aléatoire \parencite{Sharma2012213}. Il a aussi été appliqué pour
résoudre des problèmes à objectifs multiples, en combinaison avec un algorithme
évolutionnaire \parencite{Ma201448}, ou encore avec le PSO \parencite{Gao2013114}.

L’approche est ainsi retenue afin d’améliorer la diversité de la population initiale
et sera aussi utilisé durant la phase des exploratrices afin d’augmenter la qualité
de la nouvelle position.
Pour chaque source abandonnée, une position candidate ($\vec{x}$) et son opposée
($\vec{\check{x}}$) sont ainsi évaluées. Une sélection par tournoi binaire permet
de retenir la meilleure des deux qui devient alors la nouvelle position de la source.
% subsection apprentissage_par_vecteur_oppose (end)


\subsection[L’epsilon-dominance]{L’$\epsilon$-dominance} % (fold)
\label{sub:l_epsilon_dominance}
Contrairement à une approche mono-objectif, il existe plusieurs solutions optimales
dans une approche multi-objectif. Il apparait alors nécessaire de conserver l’évolution
du front de Pareto : c’est le rôle de l’archive (\munsure{Lien vers figure}).

\iunsure{Ajouter graphe décrivant l’utilité de l’archive pour la recherche \cite{Laumanns2002263}}

Afin de garantir la convergence il est nécessaire d’utiliser une approche élitiste
en limitant la population de l’archive aux solutions les plus performantes \parencite{Zitzler2000173}.
De plus afin d’éviter de converger vers un front local, la population doit conserver
une bonne diversité.
L’algorithme de base du NSGA-II \parencite{Deb2002182}, utilise le Fast Non
Dominated Sorting Algorithm afin de construire son archive. Dans cette approche
une sélection par rang est appliquée afin de trier les solutions en différents
niveaux et la distance dite de crowding (distance moyenne entre deux solutions)
est utilisée afin de maintenir la diversité.
L’algorithme SPEA II cherche à améliore la diversité du front final au prix d’une convergence
plus lente en utilisant une approche par clustering. $N$ clusters (taille de l’archive)
sont formés en se basant sur la distance euclidienne et une seule solution est retenue
pour chaque cluster.
\cite{Laumanns2002263} propose de séparer l’espace des solutions en une grille en ajoutant la notion
de $\epsilon$-dominance. Dans cette approche l’espace des objectifs est divisé en hypercubes,
et chaque hypercube est comparé selon l’$\epsilon$-dominance (Définition\ref{def:eps_dominance}).
Bien que d’autres approches utilisent une grille similaire (PAES, \cite{Knowles2000149}
l’$\epsilon$-dominance n’accepte qu’une unique solution par hypercube assurant une
plus grande diversité de la population. La taille de l’archive est donc fonction
des $\epsilon$ choisis pour chaque objectif mais est toujours finie.


\begin{Def}[$\epsilon$-dominance]\label{def:eps_dominance}
L’$\epsilon$-dominance assume que deux solutions sont identiques lorsqu’elles appartiennent
au même hypercube dont les dimensions des arrêtes sont définies par
$\vec{\epsilon}_{i}, i \in \{1, ..., M\}$ avec $M$ le nombre de fonctions objectif.
Dans le cas d’une maximisation il est dit que $\vec{x}_{a}$ $\epsilon$-domine $\vec{x}_{b}$ si :
\begin{align*}
  \forall j \in \{1, M\}, \qquad \left\lceil\frac{f_{j}(\vec{x}_{a})}{\epsilon}\right\rceil &\leq \left\lceil\frac{f_{j}(\vec{x}_{b})}{\epsilon}\right\rceil  \\
  \intertext{et}
  \exists i \in \{1, M\}, \qquad \left\lceil\frac{f_{i}(\vec{x}_{a})}{\epsilon}\right\rceil &< \left\lceil\frac{f_{i}(\vec{x}_{b})}{\epsilon}\right\rceil  \\
\end{align*}
Cette relation sera notée sera notée : $\vec{x}_{a} \prec_{\epsilon} \vec{x}_{b}$.
\end{Def}

Une solution peut ainsi être ou dominée, dominante, identique, ou non-dominée au
sens de l’$\epsilon$-dominance, les solutions $\epsilon$-non-dominées formant alors
le front d’$\epsilon$-Pareto (Fig.~\ref{fig:epsilon_dominance}).
Lors de l’ajout d’une solution à l’archive l’$\epsilon$-dominance est alors utilisé
dans un premier temps afin de les départager.
Dans le cas où une nouvelle solution serait $\epsilon$-identique elle est uniquement
comparée avec celle déjà archivée dans le même hypercube en utilisant la distance euclidienne.
La solution retenue est celle la plus proche de l’optimum de l’hypercube (coin supérieur
droit dans le cas d’une maximisation).
Contrairement à une sélection par dominance stricte le calcul de la distance euclidienne
permet de tenir compte de l’ensemble des amélioration et est moins élitiste.
Afin d’éviter l’ajout de biais, les objectifs sont normalisés dans l’espace de l’hypercube.
L’$\epsilon$-archive permet ainsi d’assurer :
\begin{itemize}
  \item L’élitisme : seule les solutions non-dominées sont retenues
  \item La diversification : une unique solution par hypercube
  \item Une taille limite : La taille maximale est fonction des $\epsilon$ et est finie
  \item Une intensification : En réduisant les $\epsilon$ on augmente la taille maximale
  \item Une épuration : En augmentant les $\epsilon$ on réduit la taille maximale
\end{itemize}

\begin{figure}
    \begin{center}
        \includegraphics{abc/selection_boxes.png}
    \end{center}
    \caption{Principe de la mise à jour de l’archive par epsilon-dominance (maximisation assumée).
             Adapté de \cite{Deb2005501}.
             \label{fig:epsilon_dominance}}
\end{figure}

L’étude réalisé par \cite{Deb2005501} montre que cette méthode d’archivage permet
d’obtenir une très bonne convergence et une grande diversité sur le front final.
Les résultats indiquent que les approches PESA et NSGA II obtiennent une moins
bonne représentation du front de Pareto que celles utilisant l’$\epsilon$-dominance
($\epsilon$-MOEA) ou le clustering (C-NSGA II, SPEA).
Cette observation est d’autant plus vrai pour des problèmes ayant plus de deux
objectifs (DTLZ1, ..., DTLZ7).
De plus les résultats mettent aussi en évidence que le NSGA II et le $\epsilon$-MOEA
converge plus rapidement que le SPEA II.
Un désavantage à cette méthode peut cependant être noté. De part sa formulation
les solutions au niveau des extrêmes sur le front de Pareto sont plus difficilement
atteignables.

L’$\epsilon$-archive est donc retenue comme solution d’archivage dans ces travaux
car il représente un bon compromis entre diversité et convergence.
% subsection l_epsilon_dominance (end)


% ------------------------------------------------------------------------------
\subsection{Tenir compte des contraintes} % (fold)
\label{sub:tenir_compte_des_contraintes}
Les problèmes multi-objectif dans le domaine de l’ingénierie sont souvent
dépendantes de contraintes limitant l’ensemble des solution optimales au domaine
de faisabilité. L’optimisation consiste alors à améliorer les objectifs tout en
respectant les contraintes spécifiques au problème \eqref{eq:def_optimisation}.

La littérature introduit de nombreuses méthodes permettant de tenir compte des
contraintes. L’approche la plus courante est d’utiliser un facteur de pénalité.
Ce facteur est dit statique si il est fixe durant l’ensemble de l’optimisation, dynamique
si il est adapté en fonction du nombre d’itération, et adaptatif si les informations
acquises par la recherche aide à sa détermination \parencite{Coello2002}.
Parmi les approches existante il peut être cité la méthode de la peine de mort
qui consiste à appliquer une pénalité très importante afin de rejeter les solutions
sous contraintes. Certaines moins exclusives désavantagent les solutions ne respectant
pas les contraintes en fonction de leur niveau de violation. D’autres approches
ajustent dynamiquement la pénalité en fonction du nombre d’itérations de l’algorithme. Le
problème récurrent avec ces approches réside dans l’ajout de paramètres supplémentaires
devant être déterminés empiriquement.

\cite{Coello2002} explique qu’une bonne prise en compte des contraintes ne devraient
pas nécessité le paramétrage de facteurs. Il explique aussi que ces facteurs impactent
fortement la performance de la méthode de prise en compte des contraintes. De plus
la méthode sélectionnée ne devrait pas demandé un nombre plus important d’évaluation
qui peuvent être très coûteuses. \cite{Deb2000311} propose une méthode de
sélection par tournoi dont les règles sont les suivantes :
\begin{itemize}
  \item Une solution avec contraintes est inférieure à une solution sans contraintes
  \item Si les deux solutions sont sans contraintes la solution dominante est préférée
  \item Si les deux solutions sont sous contraintes, la solution violant le moins les contraintes est préférée
\end{itemize}
La méthode a l’avantage d’être simple à mettre en place et est utilisé pour de nombreux
cas d’application (\mtodo{Ajouter des ref dans le bâtiment}). La méthode est cependant
très élitiste car les solutions non faisables sont toujours écartées durant la
recherche.

Dans ces travaux, une méthode adaptative moins élitiste est retenue. Adaptée
aux problèmes multi-objectif par \cite{Woldesenbet20073077}, la méthode ne demande
aucun paramétrage, et l’objectif modifié est calculé de la manière suivante :
\begin{itemize}
  \item Normaliser les objectifs selon \eqref{eq:norm_obj}
  \item Normaliser les contraintes et les agréger pour former une unique contrainte selon \eqref{eq:norm_contrainte}
  \item Calculer les objectifs modifiés par agrégation des contraintes et des objectifs selon \eqref{eq:calc_modif_obj}
\end{itemize}

\begin{align}\label{eq:norm_obj}
  \tilde{f}_{i}(\vec{x}) &= \begin{cases}
    \frac{f_{i}(\vec{x}) - min(f_{i}(\vec{x}))}{max(f_{i}(\vec{x})) - min(f_{i}(\vec{x}))} & \text{(minimisation)} \\ \\
    1 - \left[\frac{f_{i}(\vec{x}) - min(f_{i}(\vec{x}))}{max(f_{i}(\vec{x})) - min(f_{i}(\vec{x}))}\right] & \text{(maximisation)} \\
  \end{cases}
\end{align}
avec $f_{i}(\vec{x})$ la valeur de l’objectif $i$, pour la position $\vec{x}$ dans l’espace de décision.
\iunsure{Détailler pourquoi différence entre maximisation et minimisation}

\begin{equation}\label{eq:norm_contrainte}
  v(\vec{x}) = \frac{1}{J+K} \sum_{j=1}^{J+K} \left(\frac{g_{j}(\vec{x})}{max(g_{j}(\vec{x}))}\right)
\end{equation}
avec $J$ le nombre de contraintes d’inégalités et $K$ le nombre de contrainte d’égalités
transformées en contraintes d’inégalités. $g_{j}$ représente alors la fonction contrainte
associée à la contrainte $j$ avec $g_{j} \geq 0$.

\begin{equation}\label{eq:calc_modif_obj}
  F_{i}(\vec{x}) = d_{i}(\vec{x}) + p_{i}(\vec{x})
\end{equation}
avec $d_{i}(\vec{x})$ calculé selon \eqref{eq:dist_obj} et $ p_{i}(\vec{x})$ selon \eqref{eq:penalty_norm}.


\begin{align}\label{eq:dist_obj}
  d_{i}(\vec{x}) = \begin{cases}
                          v(\vec{x})                                     & \qquad si\  r_{f} = 0 \\
                          \sqrt{v(\vec{x})^2 + \tilde{f}_{i}(\vec{x})^2} & \qquad sinon          \\
                    \end{cases}
\end{align}
\begin{equation}\label{eq:penalty_norm}
  p_{i}(\vec{x}) = (1 - r_{f})  X(\vec{x}) + r_{f} Y_{i}(\vec{x}) \\
\end{equation}

\begin{align*}
  \shortintertext{où} \\
    r_{f} &= \frac{\text{Nombre de solution réalisables}}{\text{Taille de la population}} \\
  \shortintertext{et} \\
  X(\vec{x})     &= \begin{cases}
                0,          \qquad     & si\  r_{f} = 0\\
                v(\vec{x}), \qquad     & sinon\\
                \end{cases} \\
  Y_{i}(\vec{x}) &= \begin{cases}
                    0,          \quad \qquad & \text{si} \ \forall j, \ g_{j}(\vec{x}) = 0\\
                      \tilde{f}_{i}(\vec{x})  \\
            \end{cases}\\
\end{align*}

L’algorithme s’adapte ainsi dynamiquement à la population et peut accepter des
solutions ne respectant pas les contraintes. Plus il y a de solutions non
réalisables dans la population, moins une solution ne respectant pas les contraintes à
de chances d’être sélectionnée. L’approche permet d’améliorer l’exploration de
l’espace de décision et ainsi de pouvoir trouver des solutions existantes même
lorsque l’espace de faisabilité est faible.
L’approche est donc retenue dans ces travaux afin de mettre à jour la position des
sources mais les solutions de l’archive elles n’acceptent que des solutions respectant
l’ensemble des contraintes.
% subsection tenir_compte_des_contraintes (end)


% ------------------------------------------------------------------------------
\subsection{Description de l’approche globale} % (fold)
\label{sub:description_de_l_approche_globale}
Dans la section précédente, chaque élément utilisé dans l’algorithme a été détaillé et
les avantages et inconvénients des approches retenues ont aussi été explicités.
Dans cette section le fonctionnement global de la méthode approchée d’optimisation
multi-objectif retenue est décrite.

L’algorithme peut être définie par Fig.~\ref{fig:abc_complet}.
Les sources sont initialisées (Algorithm~\ref{alg:init_phase}) suivant la méthode
OBL (Définition\ref{def:oblm}) afin d’obtenir une meilleure représentation du
domaine de décision en amont de l’optimisation.
Durant la phase des butineuses (Algorithm~\ref{alg:employed_phase}), chaque source
subit une variation en tenant compte de deux sources, une de l’essaim et une de l’archive.
Durant la phase des ouvrières (Algorithm~\ref{alg:onlooker_phase}), la qualité de
chaque source \eqref{eq:attribution_prob_to_source} est utilisée afin de sélectionner
et d’exploiter seulement les plus prometteuses. Finalement, si une source a subit le
nombre maximum d’échec ($MaxEchec$) alors une exploratrice (Algorithm~\ref{alg:scout_phase})
réinitialise sa position de manière aléatoire.
Tant que la condition d’arrêt n’est pas atteinte, les phases des butineuses,
des ouvrières, et des exploratrices sont répétées de manière cyclique. Au cours de
la recherche les nouvelles sources sont ajoutées à l’archive utilisée
comme élément d’apprentissage pour les butineuses et les ouvrières mais pas pour les
exploratrices. Une fois la condition d’arrêt atteinte l’ensemble des solutions
de l’archive forme le front de Pareto.

\begin{figure}
    \begin{center}
        \includegraphics[width=10cm]{abc/algorithme_complet.pdf}
    \end{center}
    \caption{Description globale de l’algorithme ABC pour les problèmes multi-objectif.
             \label{fig:abc_complet}}
\end{figure}

Dans l’optique de l’optimisation d’un modèle solaire développé sous \emph{Dymola}, l’algorithme
a été implémenté en \emph{Python} afin de faciliter le couplage avec la bibliothèque
\emph{BuildingsPy} qui comme nous l’avons vu dans le chapitre précédent permet d’automatiser
la simulation de modèle \emph{Modelica} sous \emph{Dymola}. La bibliothèque est
disponible sous le nom de \emph{pyMOABC} et implémente :
\begin{itemize}
  \item une $\epsilon$-archive et la box-dominance
  \item la dominance au sens de Pareto
  \item la prise en compte des contraintes suivant \cite{Woldesenbet20073077}
  \item une interface commune permettant l’abstraction du type de variable (discrète, continue, qualitative)
  \item une représentation des solutions (sources et abeilles)
  \item l’algorithme tel que décrit dans ces travaux
  \item une suite de test unitaires et des cas théorique validant l’approche avec
        et sans contraintes
\end{itemize}

L’approche retenue contrairement à la majorité des méta-heuristiques demande
peu de paramètre à définir de manière empirique et est ainsi robuste. En effet
seule la taille de la population et le nombre d’essai maximum sont à définir.
De plus la taille de la population n’impacte pas le nombre de solutions dans l’archive
car le stockage est réalisé dans une structure indépendante. Il est aussi important
de noter que les deux uniques paramètres on un comportement prévisible facilitant
leur paramétrage. La taille de la population en augmentant améliore l’exploration réduisant le risque
de se bloquer dans les optimums locaux mais réduit la vitesse de convergence (Définition\ref{def:convergence}).
Le nombre d’essai maximum lui est directement lié au nombre de variable de décision variant
dans l’espace de décision et à la taille de la population. Une valeur faible traduisant
l’abandon rapide d’une solution. Ainsi on note clairement que la taille de la population
impacte fortement l’exploration là où le nombre d’essai maximum impacte l’exploitation.

% Phase d’initialisation
\begin{algorithm}\label{alg:init_phase}
  \SetAlgoVlined
  \DontPrintSemicolon
  \emph{Initialisation des \ASources sur l’ensemble de l’espace de décision}\;
  \For{$i \leftarrow 0$ \KwTo \ANbrSources}
  {
    \emph{Initialisation des variables de décision pour chaque source}\;
    \For{$j \leftarrow 0$ \KwTo \ANbrVariables}
    {
      \encircle{a} \emph{Initialisation de la position de la $\ASource_{i}$ de manière aléatoire}\;
      \Indp
      $x_{ij} = x_{j}^{min} + RandUniform(0, 1) \times (x_{j}^{max} - x_{j}^{min})$\;
      \Indm
      \BlankLine
      \encircle{b}  \emph{Génération de l’$\ABee_{i}$ dont la position est générée suivant Définition\ref{def:oblm}}\;
      \Indp
      $ \check{x_{ij}} = x_{j}^{min} + x_{j}^{max} - x_{ij}$\;
      \Indm
      \BlankLine
      \encircle{c} \emph{Évaluation de la $\ASource_{i}$ et de l’$\ABee_{i}$}\;
      \BlankLine
      avec $RandUniform$ un tirage aléatoire suivant une loi uniforme, et $x_{j}^{min}$, $x_{j}^{max}$
      respectivement le minimum et le maximum de la variable $j$\;
    }
    \If{$\ASource_{i}$ respecte toutes les contraintes}
    {
      $\AArchive \pluseq \ASource_{i}$ \AComment{On ajoute la source initial à l’archive}
    }
    \If{$\ABee_{i}$ respecte toutes les contraintes}
    {
      $\AArchive \pluseq \ABee_{i}$ \AComment{On ajoute la source opposée à l’archive}
    }
  }
  Mise à jour des \ASources d’après Algorithm~\ref{alg:maj_phase}\;
  \caption{Initialisation des sources par OBLM (Définition\ref{def:oblm}).}
\end{algorithm}

% Phase des éclaireuses
\begin{algorithm}\label{alg:scout_phase}
  \SetAlgoVlined
  \DontPrintSemicolon
  \AComment{Exploration par les \AScouts}
  \For{$i \leftarrow 0$ \KwTo \ANbrSources}
  {
    \If{$\ATrial_{i} > \AMaxTrial$ }
    {
      Génération de deux nouvelles positions suivant Algorithm~\ref{alg:init_phase}\;
    }
  }
  Mise à jour de la position des \ASources d’après Algorithm~\ref{alg:maj_phase}\;
  \caption{Phase des éclaireuses.}
\end{algorithm}


% Phase des ouvrières
\begin{algorithm}\label{alg:onlooker_phase}
  \SetAlgoVlined
  \DontPrintSemicolon
  \AComment{Exploitation des sources par les \AOnlookers}
  \AComment{Plusieurs \AOnlookers peuvent modifier la même source}
  \For{$\AOuvriere \in \AOnlookers$}
    {
      Sélection aléatoire d’une \ASource $i$ selon la probabilité
      définie par l’équation \eqref{eq:attribution_prob_to_source}\;
      Sélection aléatoire d’une source $k$ ($k \neq i$) dans l’\AArchive\;
      \encircle{a} \emph{Génération d’une nouvelle position $\check{\vec{x_{i}}}$ à partir de la
                         position $\vec{x_{i}}$ pour l’\AOuvriere }\;
      \For{$j \leftarrow 0$ \KwTo \ANbrVariables}
      {
      \begin{algomathdisplay}
        \check{x_{ij}} =%
          \begin{cases}
            x_{ij}  + RandUniform(-1, 1)   \times \ (x_{ij} - x_{kj}) &\ \ATirageB < \AMR \\
            x_{ij}                                                    &\ sinon
          \end{cases}
      \end{algomathdisplay}
      }
      \BlankLine
      Avec \AMR est la probabilité de réaliser une modification (fixée à 0,5) et
      \ATirageB un nombre aléatoire entre 0 et 1\;
      \BlankLine
      \encircle{b} \emph{Évaluation des objectifs pour la nouvelle position $\check{\vec{x_{i}}}$}\;
      \BlankLine
      \If{\AOuvriere respecte toutes les contraintes}
      {
        $\AArchive \pluseq \AOuvriere$ \AComment{Ajout de la solution trouvée par l’\AOuvriere à l’archive}
      }
    }

  Mise à jour de la position des \ASources qui ont été modifiées d’après Algorithme~\ref{alg:maj_phase}\;
  \caption{Phase des ouvrières.}
\end{algorithm}

% Maj des sources
\begin{algorithm}\label{alg:maj_phase}
  \SetAlgoVlined
  \DontPrintSemicolon
  Récupérer le maximum et minimum pour chaque objectif dans l’\AHive\;
  Récupérer le maximum pour chaque contrainte dans l’\AHive\;
  \For{$\ABee \in \ABees$}
  {
    Calcul du vecteur objectif pour l’\ABee ($\check{\vec{F}}_{i}$) et pour sa \ASource $i$ ($\vec{F}_{i}$),
    respectivement aux positions $\check{\vec{x_{i}}}$ et $\vec{x_{i}}$
    selon \eqref{eq:calc_modif_obj}\;
    \For{$m \leftarrow 1$ \KwTo M}
    {
      \begin{algomathdisplay}
      \begin{aligned}
      \check{F}_{im} &= d_{m}(\check{\vec{x_{i}}}) + p_{m}(\check{\vec{x_{i}}}) \\
      F_{im}         &= d_{m}(\vec{x_{i}}) + p_{m}(\vec{x_{i}}) \\
      \end{aligned}
      \end{algomathdisplay}
    }
    \If{$\check{\vec{x_{i}}} \succ \vec{x_{i}}$}
    {

      $\ASource_{i} \leftarrow \ABee$ \AComment{Mise à jour de la \ASource à partir de l’\ABee}

      $\ATrial_{i} \leftarrow 0$ \AComment{On réinitialise le nombre d’essais pour la \ASource $i$}
    }
    \Else
    {
      $\ATrial_{i} \pluseq 1$ \AComment{On incrémente le nombre d’essais pour la \ASource $i$}
    }
  }
  \caption{Mise à jour des \textbf{Sources} par les \textbf{Abeilles}}
\end{algorithm}

% Phase des butineuses
\begin{algorithm}\label{alg:employed_phase}
  \SetAlgoVlined
  \DontPrintSemicolon
  \AComment{Exploration des sources par les \AEmployed}
  \For{$i \leftarrow 0$ \KwTo \ANbrSources}
  {
    Sélection aléatoire d’une source $k$ ($k \neq i$) dans l’\AArchive\;
    Sélection aléatoire d’une source $p$ ($p \neq i$) dans l’\AHive\;
    \BlankLine
    \encircle{a} \emph{Génération d’une nouvelle position $\check{\vec{x_{i}}}$ à partir de la %
                       position $\vec{x_{i}}$ pour la \AButineuse $i$}\;
    \If{$\ATirageA < \ARatio $ }
      {
      \For{$j \leftarrow 0$ \KwTo \ANbrVariables}
      {
      \begin{algomathdisplay}
        \check{x_{ij}} =%
          \begin{cases}
            \begin{aligned}
              x_{ij}  &+ 0.01 \times  \ALevy  &\times \ (x_{ij} - x_{pj})  \\
                      &+ 0.01 \times |\ALevy| &\times \ (x_{kj} - x_{ij})  \\
            \end{aligned} &\ \ATirageB < \AMR \\
            x_{ij}        &\ sinon
          \end{cases}
      \end{algomathdisplay}
      \ALevy est nombre aléatoire dans une distribution de Lévy
      permettant de réaliser un vol de Lévy (Définition\ref{def:vol_levy})\;
      }
      }
    \Else
      {
      \For{$j \leftarrow 0$ \KwTo \ANbrVariables}
      {
      \begin{algomathdisplay}
        \check{x_{ij}} =%
          \begin{cases}
            \begin{aligned}
              x_{ij}  &+ RandUniform(-1, 1)   &\times \ (x_{ij} - x_{pj})  \\
                      &+ RandUniform(0, 1)    &\times \ (x_{kj} - x_{ij})  \\
            \end{aligned} &\ \ATirageB < \AMR \\
            x_{ij}        &\ sinon
          \end{cases}
      \end{algomathdisplay}
      }
      }
      Où \ARatio est la probabilité de réaliser un vol de Lévy (fixée à 0,5), et \AMR la probabilité
      de réaliser une modification (fixée à 0,8). \ATirageA et \ATirageB étant des nombres aléatoires
      (entre 0 et 1) tirés dans une distribution uniforme\;
      \BlankLine
    \encircle{b} \emph{Évaluation des objectifs pour la nouvelle position $\check{\vec{x_{i}}}$}\;
    \If{$\AButineuse_{i}$ respecte toutes les contraintes}
    {
      \AComment{On ajoute la nouvelle source à l’archive}
      $\AArchive \pluseq \AButineuse_{i}$\;
    }
  }
  \AComment{On ne conserve que une seule position par source}
  Mise à jour de la position des \ASources d’après Algorithme~\ref{alg:maj_phase}\;
  \caption{Phase des butineuses.}
\end{algorithm}
\FloatBarrier
% subsection description_de_l_approche_globale (end)


% ------------------------------------------------------------------------------
\subsection{Validation de la méthode} % (fold)
\label{sub:validation_de_la_methode}
Cette section permet d’apprécier la performance de l’approche pour différent types
de problèmes pouvant être rencontrés dans un problème d’optimisation.

\itodo{Cette section est en construction}

Premièrement la convergence vers le front de Pareto a été vérifié à travers de nombreux
problèmes d’optimisation multi-objectif comportant ou non des contraintes.
Dans cette optique la bibliothèque \emph{pyMOABC} a été développée autour de tests unitaires
(Test-driven development).
Chaque éléments (fonction / méthodes / ...) et comportements (minimisation / maximisation / ...)
ont ainsi été testés afin de garantir que la bibliothèque implémente correctement
les éléments et comportements nécessaires. De plus une fois le test réussi son existence
permet de garantir que les modifications futures respecteront l’ensemble des contraintes
déjà existantes.
Cette approche couplée aux problèmes étalon existants a ainsi permis de valider
l’implémentation de l’approche. Bien que la réussite d’un problème étalon ne soit
pas une condition suffisante pour garantir la convergence sur un problème réel,
il apporte une information importante : la capacité du méta-heuristique à s’adapter
à différent type de contraintes. Dans cette optique les résultats
qui seront présentés dans cette section sont sélectionnés afin d’évaluer la performance
de l’algorithme sur des étalons couvrant les principales difficultés existantes.

Les problèmes retenus sont ainsi les suivants :
\begin{itemize}
  \item front convexe : ZDT1 (2 objectifs)
  \item front linéaire : Hanne 1 (2 objectifs)
  \item front concave : ZDT2 (2 objectifs)
  \item multi-modal : ZDT4 (2 objectifs)
  \item Non-uniformité avec faible densité près de l’optimal : DTLZ5, ZDT6, ConvexGlobalNonConvexLocalHive
  \item Discontinue : DTLZ6, Kursawe
\end{itemize}

La méthode retenue pour la prise en compte des contraintes a déjà été testé et
comparée par l’auteur (\mtodo{Ajouter ref}) et ne font donc pas parti des tests
retenus.


% subsection validation_de_la_methode (end)
% section artificial_bee_colony (end)




% ..............................................................................
% ..............................................................................
\section{Réduction de la cardinalité du problème} % (fold)
\label{sec:reduction_de_la_cardinalite_du_probleme}

\iunsure{Cette partie devrait être dans l’étude de cas ???}
Comme il a été décrit dans la section précédente, l’optimisation multi-critère de
problème mixte est complexe et coûteux. Pour cette raison les méta-heuristiques
ont été développé réduisant le nombre d’évaluation nécessaire par rapport à une
approche exacte.
Cependant le domaine de décision a priori est aussi un facteur impactant sur le
nombre d’évaluation nécessaire pour obtenir le front de Pareto lors de l’optimisation.
Afin de limiter la complexité d’un problème, des méthodes dite d’analyse de sensibilité
ont ainsi été développées.
Dans \cite{Iooss2011} une distinction est faite entre les méthodes dites locales
(évaluant les effets à partir d’un autour d’une position) et les méthodes globales qui
s’intéressent à l’ensemble du domaine de définition. Afin de pouvoir caractériser
un domaine de décision le plus petit possible, il est nécessaire d’avoir recours à
ces méthodes globales.
Ces méthodes permettent ainsi d’identifier et sélectionner les facteurs les plus influents
au regard des objectifs retenues. Les facteurs non influent étant transformés en
constante réduisant la cardinalité du problème. Le domaine de décision est ainsi
réduit et le processus d’optimisation plus performant et moins couteux.

\iunsure{Ajouter les graphes de Iooss montrant le classement des méthodes}
\cite{Iooss2011} regroupe les méthodes existantes en deux grande familles : les
méthodes de criblage et les méthodes de la décomposition de la variance. Il propose
aussi un diagramme de décision permettant de sélectionner la méthode en fonction
du temps nécessaire pour une évaluation et du nombre de variables d’entrées.

Dans notre cas, l’analyse de sensibilité choisie doit être globale, peu couteuse
et identifier les facteurs non influents. Un classement quantitatif n’étant pas
nécessaire, les méthodes de criblage et plus particulièrement la méthode de Morris
a été sélectionnée.



% ------------------------------------------------------------------------------
\subsection{La méthode de Morris} % (fold)
\label{sub:la_methode_de_morris}

\itodo{Présenter l’analyse de sensibilité choisie (graphique, description, ...)}
Il existe plusieurs méthodes de criblage mais la méthode de Morris \parencite{Morris1991161}
est facilement adaptable à tout type de problèmes contrairement à d’autres approches
comportant plus de contraintes. Cette méthode est par exemple toujours valide dans le cas où
les facteurs se compenseraient ou encore si le signe de l’effet d’un facteur n’est pas
connue a priori \parencite{Saltelli2004}.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Principe} % (fold)
\label{ssub:principe}
La méthode de Morris utilise un plan OAT (One (Factor) At the Time) et est basée sur
l’EEM (Elementary Effect Method) \parencite{Saltelli2004}.
Chaque facteur (entrée) est définie par une plage de variation discrétisée en fonction
du nombre de \emph{niveaux} ($p$) et d’un intervalle définie par un \emph{pas} ($\delta$).
Chaque facteur assume ainsi $p$ valeurs discrètes et $\delta$ doit être un multiple de
$\frac{1}{(p - 1)}$. La littérature utilise couramment $\delta = \frac{p}{2 \times (p - 1)}$
\parencite{Morris1991161, Campolongo20071509}.

La méthode consiste alors en l’évaluation de l’effet élémentaire pour $r$ répétitions
(trajectoires) à l’intérieur du plan OAT. Pour chaque trajectoire, une position de départ
est tirée aléatoirement puis des variations élémentaires aléatoires (distribution uniforme)
sont réalisées successivement. La méthode de Morris demande ainsi $r \times (k + 1)$ évaluations
avec $k$ le nombre de facteur dont on cherche à évaluer l’influence. Pour chaque trajectoire
on calcule les effets élémentaires sur le ou les objectifs et ce pour chaque facteurs $k$.
Ainsi la précision de l’analyse est fonction du nombre de trajectoire mais aussi
de leur diversité qui est définie par le choix des paramètres $r$ et $p$.
Encore une fois la littérature suggère $r \geq 10$ et de $p = 4$.

\ftodo{Ajouter un graphe explicitant la recherche (cube de Morris, ..)}
% subsubsection principe (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Interprétation} % (fold)
\label{ssub:interpretation}
La première implémentation de la méthode permet d’évaluer qualitativement l’influence de chaque
paramètre grâce à la moyenne, $\mu$ \eqref{eq:moyenne} et à l’écart type, $\sigma$ \eqref{eq:ecart_type}.
La moyenne permettant d’évaluer les effets linéaires chaque paramètre sur les indicateurs,
et l’écart type lui d’identifier les influences non linéaires ou les couplages entre différents facteurs.

\begin{equation}\label{eq:moyenne}
    \mu = \sum_{i = 1}^{r} \frac{d_{i}}{r}
\end{equation}

\begin{equation}\label{eq:ecart_type}
    \sigma = \sqrt{\sum_{i=1}^{r}\frac{(d_{i} - \mu)^{2}}{r}}
\end{equation}

\cite{Campolongo20071509} améliore la méthode en proposant deux modifications. La première
est l’ajout de la moyenne absolue, $\mu^{*}$ \eqref{eq:moyenne_absolue}. Cet indicateur permet
de voir l’importance des facteurs dont le signe de l’influence est variable là où
la moyenne ne donnerais pas l’information (à cause des compensations dues au signes).
La seconde modification proposée est la génération d’un grand nombre de trajectoires, $N$,
dans lesquelles $r$ trajectoires sont sélectionnées. La méthode permet ainsi
d’obtenir une meilleure répartition des trajectoires.
La méthode de sélection proposée est faite par \emph{Brute force} et demande une
importante puissance de calcul lorsque le nombre de trajectoires augmente.

\begin{equation}\label{eq:moyenne_absolue}
    \mu^{*} = \sum_{i = 1}^{r} \frac{\lvert d_{i} \rvert}{r}
\end{equation}

Afin de palier à ce problème, \cite{Ruano2012103} propose une approximation de
la meilleure répartition dans un temps de calcul très court permettant d’obtenir
une sélection représentatif du domaine d’exploitation.

Il est ainsi possible de classer les facteurs en 3 catégories :
\begin{itemize}
  \item Non-influent ou effets négligeables
  \item Influent avec des effets sans interactions et linéaires
  \item Influent avec des effets non-linéaire ou des interactions
\end{itemize}
% subsubsection interpretation (end)


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Analyse} % (fold)
\label{ssub:analyse}
La méthode recommandée dans la littérature pour évaluer les résultats de l’analyse
est graphique et consiste à tracer le plan ($\sigma$, $\mu$ ou $\mu^{*}$). Il est
ainsi possible de juger de l’importance globale d’un facteur visuellement.
Enfin la distance normalisée notée $\hat{distance}$ \eqref{eq:distance_norm}, permet
de classer l’ensemble des indicateurs sur un même graphe à l’aide d’une représentation
en diagramme à barre renseignant l’ordre au sein des facteurs évalués.
Il est cependant important de ne pas oublier que $\hat{distance}$ est une distance
normalisée, il existera toujours un facteur ayant une $\hat{distance} = 1$ et un autre
$\hat{distance} = 0$. Ces deux méthodes sont ainsi complémentaires.

\begin{align}\label{eq:distance_norm}
    \begin{split}
        distance        &= \sqrt{{\mu^{*}}^2 + \sigma^{2}} \\
        \hat{distance}  &=  \frac{distance - distance_{min}}{distance_{max} - distance_{min}}
    \end{split}
\end{align}

\ftodo{Ajouter un graphe explicitant ces deux approches}
% subsubsection analyse (end)
% subsection la_methode_de_morris (end)
% section reduction_de_la_cardinalite_du_probleme (end)





































% % ------------------------------------------------------------------------------
% \subsection{Introduction} % (fold)
% \label{sub:introduction}
% \itodo{À reprendre complètement}

% L’optimisation multi-objectif est aujourd’hui largement utilisé dans le bâtiment.
% On a par exemple \cite{Armand-Decker2015} qui a développé une méthode d’optimisation pour les
% construction bois en utilisant un meta-modèle de bâtiment. Elle utilise un meta-heuristique
% à population (Particule Swarm optimization) et évalue les besoins en énergie, le confort des
% occupants, la sécurité de l’ouvrage et de l’impact environnemental. \cite{Rivallain2013}
% a quand à lui utilisé une méthode approchée (NSGA-II) et une exacte (programmation dynamique)
% pour identifier des programmes séquentiels efficaces de réhabilitation énergétique.
% Il a ainsi optimisé la combinaison des modifications pour chaque phase mais aussi l’ordre
% dans lequel ces améliorations doivent être réalisées afin d’être le plus optimal
% possible.
% Pour les différentes solutions l’impact environnemental, le confort des occupants en
% période estivale, et le coût ont été évaluées.
% \itodo{Ajouter des sources vers des exemples d’optimisation de bâtiment}

% On a aussi de nombreux exemples d’optimisation de système énergétique.
% \itodo{Ajouter des sources vers des exemples d’optimisation de système}

% On voit donc que l’optimisation est un outil qui a été largement utilisé dans
% le bâtiment comme pour l’amélioration ou l’identification de solutions performantes
% pour les systèmes.

% \itodo{Ajouter du bla bla sur l’originalité et les perspectives de ces travaux}
% L’originalité de ce travail provient principalement du couplage entre le bâtiment
% et ses systèmes. La plupart des optimisations de bâtiment évalues les besoins
% du bâtiment et considère donc un système de chauffage et de ventilation idéaux.
% Dans ces travaux on cherche à optimiser la partie système et son algorithme de
% contrôle en même temps que l’enveloppe du bâtiment. On évalue donc plus un besoin
% en énergie mais une consommation qui est fonction de la performance des systèmes
% envisagées. Les travaux se concentre sur l’évaluation de solutions utilisant fortement
% l’énergie solaire comme vecteur énergétique. On cherche donc à couvrir les besoins
% de chauffage, d’eau chaude sanitaire (ECS) et d’électricité.
% Cette approche permettra d’évaluer le potentiel d’autonomie solaire disponible
% pour différentes combinaisons systèmes/enveloppe.

% Enfin ces travaux vise à l’élaboration d’un outil d’aide à la décision. Il existe
% diverses methodes pour faire de l’aide à la décision (~\autoref{sec:vers_une_optimisation_multi_critere_et_multi_objectif}).
% L’approche choisie est de déterminer un
% ensemble de solutions non-dominées dans un premier temps, puis d’utiliser des
% outils d’aide à la décision pour réduire le nombre de solution. On se trouve donc
% dans une approche par front de Pareto.
% \itodo{Ajouter du bla bla sur le choix de l’ordre entre optimisation et aide à la décision}

% \itodo{Reformuler tout ça pour mieux introduire les parties qui suivent}
% % subsection introduction (end)
% % subsection les_approches_existantes (end)
% % section vers_une_optimisation_multi_critere_et_multi_objectif (end)



% % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% \subsubsection{Histoire de l’Artificial Bee Colony (ABC)} % (fold)
% \label{ssub:histoire_de_l_artificial_bee_colony}
% L’intelligence artificielle se traduit par la construction de programmes informatiques
% pour la réalisation de tâches demandant une démarche critique, et de l’apprentissage. La
% notion a été inventé par John McCarthy et Marvin Lee Minsky et ne cesse de s’améliorer
% dans de nombreux domaines comme le déplacement organisé de groupe important d’animaux (humains, oiseaux, poissons, ...)
% ou encore l’apprentissage et la réflexion \parencite{Hsu199970,Silver2016484}).
% Une des branches de l’intelligence artificielle s’intéresse particulièrement au comportement
% du monde animal. Dans notre cas on s’intéresse à la branche de l’intelligence des
% essaims (Swarm Intelligence). \cite{Bonabeau1999} a définit quatre caractéristiques
% définissant l’organisation dans ces essaims:
% \begin{itemize}
%     \item \emph{positive feedback}, se traduisant par le renforcement de chemins ou le recrutement d’individus suite à un constat
%           d’un des individus
%     \item \emph{negative feedback}, permettant de stabiliser la structure évitant la saturation du au positive feedback
%     \item \emph{fluctuations ou amplification}, faisant émerger des solutions nouvelles (facteur aléatoire)
%     \item \emph{multiple interactions}, pouvant se traduire par le partage d’informations entre les individus de la population
% \end{itemize}
% Les algorithmes les plus connues étant inspirées des oiseaux (Particule Swarm Intelligence),
% des fourmis (Ant Colony), ou encore des abeilles qui a été choisi pour les raisons explicité ci-avant.
% Il existe plusieurs approches d’algorithmes d’essaims d’abeilles. Certaines sont basées
% sur le comportement des butineuses faisant intervenir la fameuse danse des abeilles pour partager
% les informations sur la qualité d’une source aux autres abeilles. D’autres s’inspirent de la
% reproduction des reines ou encore du mariage.
% Parmi les plus utilisés on peut citer le mariage entre abeilles introduit par \cite{Abbass20011}, l’algorithme VirtualBee
% créé à l’origine pour l’optimisation de fonction numérique \cite{Yang2005317}, l’algorithme Bee Colony Optimization (BCO)
% \cite{Lucic2001441} pour l’optimisation de problèmes combinatoire. Enfin on peut citer les
% algorithmes BeeHive proposé par \cite{Wedde200483} et les algorithmes Artificial Bee Colony (ABC) introduit par
% \cite{Karaboga2005}. ABC simule le comportement des butineuses pour la recherche de sources prometteuses.
% Comme le montre l’état de l’art de \cite{Karaboga201221} il est l’algorithme le plus
% utilisé pour la résolution de problèmes d’optimisation et peut être appliqué à toute sorte de problème: continues,
% combinatoires, mono et multi-objectifs, contraints, ou encore pour faire du clustering.

% À l’origine pensé pour résoudre des problèmes continues il a été adapté pour traiter des problèmes
% d’optimisation binaire \cite{Kashan2012342}, combinatoire \cite{Karaboga20113021}, et pour des cas multi-objectifs
% \cite{Akbari201239,Omkar2011489}.
% Ce méta-heuristique a été utilisé pour résoudre des problèmes de tout type, dont l’entrainement de réseaux de
% neurones \parencite{Karaboga2007}), le génie électrique/mécanique/civil \parencite{Rao2009887}), ou encore le clustering \parencite{Zhang20104761}).
% On retrouve aussi cet algorithme dans l’optimisation de système de chauffage \parencite{Atashkari2011}) et dans des problèmes avec
% contraintes \parencite{Tsai201480,Karaboga20113021}).
% Malgré son jeune âge la littérature sur les colonies d’abeilles augmente exponentiellement et continue de se diversifier
% pour mieux répondre aux différents types de problèmes d’optimisations.
% \itodo{Ajouter un graphique montrant l’attrait pour ces techniques (nombre d’utilisation en cours des années) \cite{Karaboga201221}}
% % subsubsection histoire_de_l_artificial_bee_colony (end)



% % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% \subsubsection{Extensions de l’algorithme original} % (fold)
% \label{ssub:extensions_de_l_algorithme_original}
% \itodo{Ajouter une description des approches pour améliorer ABC à cause des problème d’exploitation (Sharma2012213 p.217 pour blabla)}
% Un meta-heuristique se compose de deux étapes principales, l’exploration et l’exploitation.
% L’exploration est mesurée par la capacité à explorer l’espace de solution à la recherche
% des zones prometteuse pour un optimum.
% Cependant une fois ces zones trouvées, une autre approche est nécessaire: l’exploitation.
% L’exploitation consiste à l’aide de l’information disponible sur le voisinage et sur la solution actuelle
% de l’améliorer et donc de converger vers un optimum.
% Afin d’éviter de tomber dans un optimum local ou bien de converger très lentement, un équilibre entre exploration et
% exploitation est nécessaire.
% Dans notre cas l’algorithme ABC est reconnue pour être bon en exploration mais faible en exploitation \parencite{Karaboga2009108,Zhu20103166,Karaboga201221}).
% En effet le paramètre $limit$ et les éclaireuses permettent d’éviter de se coincer dans un optimum local, cependant
% l’essaim n’utilise pas autant d’informations pour diriger la recherche locale, se traduisant par une convergence
% plus lente vers l’optimum.
% L’algorithme étant encore jeune de nombreuses améliorations et/ou modifications on été ajouté au cours des dernières années.
% Certaines améliorations s’inspirent du PSO en tenant compte d’un facteur d’inertie ou de
% la meilleure solution globale \parencite{Lei2010,Zou20109}). D’autres s’inspirent de algorithmes évolutionnaires
% et génétiques \parencite{Bi2011174,Zhao2010558}) où encore avec des approches hybrides dont \cite{Pulikanti2009196} qui utilise un heuristique.
% \cite{Zhu20103166} ajoute la prise en compte de la meilleure solution actuelle dans l’équation de mise à jour.
% Il nomme le nouvel algorithme Gbest Artificial Bee Colony (GABC) et la nouvelle formulation
% de la mise à jour\emtodo{Ajoute mise à jour de l’équation} est décrite ci-dessous:
% Pour contrôler l’importance de la meilleure solution actuelle un coefficient est tiré aléatoirement
% selon une distribution uniforme ($\psi_{ij}$). La borne maximale $C$ de la distribution est définie par l’utilisateur et
% l’augmenter correspond à augmenter l’importance de la meilleure solution actuelle.
% \cite{Li2012320} propose une autre variante en ajoutant une inertie, une accélération, et l’influence de la meilleure actuelle et
% le nomme Improved Artificial Bee Colony (I-ABC).
% Il propose aussi une seconde variante faisant évoluer 3 essaims avec 3 algorithmes différents (PS-ABC). Il est cependant
% important de rester prudent sur l’interprétation des résultats, particulièrement sur la variante PS-ABC. En effet comme
% le décrit \cite{Mernik2015115}, les algorithmes doivent être comparés suivant le nombre d’évaluations de la/les fonctions
% objectifs et non par rapport au nombre d’itérations. En effet la variante PS-ABC ici utilise 3 populations et réalise donc
% en moyenne 6 fois plus d’évaluation de fonction par itération que l’approche standard.
% On peut aussi citer \cite{Aderhold2010283,Karaboga2014227} qui ajoutent respectivement la distance euclidienne pour la sélection
% de solution optimales proches, et la différenciation entre butineuses et ouvrières pour la mise à jour des sources.

% Plusieurs approches ont aussi été envisagées pour les problèmes multi-objectifs. On a par exemple l’approche MO-ABC, VABC, MHABC-CMO
% \cite{Hedayatzadeh2010, Akbari201239} adapte l’algorithme ABC pour résoudre des problèmes multi-objectifs. Ils proposent
% principalement deux modifications. La première est l’utilisation de l’$\varepsilon$-dominance
% (~\autoref{sub:mise_a_jour_du_front_de_pareto})
% pour la gestion de l’archive assurant ainsi la diversité des solutions. La seconde modifie la mise à jour des sources en
% exploitant la population et les solutions de l’archive.
% \cite{Zhang20121} introduit une approche basée sur plusieurs essaims se partageant les informations et une sélection
% des solutions du front de Pareto par crowding distance. Le papier propose une comparaison avec d’autres approches sur un ensemble de
% problèmes multi-objectif avec contraintes.
% Enfin on peut aussi noter l’approche de \cite{Omkar2011489} qui cherche à optimiser chaque objectifs séparément et nomme l’approche
% Vector Evaluated Artificial Bee Colony (VEABC). Chaque individus d’un
% essaim sont mis à jour en utilisant les individus des autres essaims. C’est cette collaboration qui permet à l’algorithme
% de trouver l’espace de compromis.
% % subsubsection extensions_de_l_algorithme_original (end)
% % subsection generation_de_solutions_quelle_methode (end)


% % - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
% \subsubsection{Vol de Lévy} % (fold)
% \label{ssub:vol_de_levy}
% C’est cette dernière qui nous intéresse plus particulièrement pour une application en méta-heuristique.
% Il a en effet été observé chez diverses espèces comme les atèles (singes), les albatros,
% ou encore la famille des Tephritidae (petites mouches) un comportement respectant
% une marche aléatoire connu sous le nom de Lévy Flight \parencite{Sharma2012213,Yang201445}).
% Cette marche aléatoire est une marche aléatoire utilisant une distribution de Lévy pour
% déterminer la longueur du pas aléatoirement.
% La distribution de Lévy de son auteur Paul Lévy est une distribution à queue lourde, indiquant que son comportement
% éloignée de la zone centrale de la distribution n’est pas exponentiellement bornée.
% La loi de Lévy dépend de deux paramètres: $\nu > 0$ qui est le pas minimum et $\gamma > 0$ qui est le
% facteur d’échelle.
% La densité de probabilités peut ainsi s’écrire sous la forme analytique simplifiée suivante \parencite{Yang201445}):
% \begin{align}\label{eq:dens_levy}
%     L(s, \gamma, \mu) = \begin{cases}
%                             \sqrt{\frac{\gamma}{2\pi}} \exp\left[-\frac{\gamma}{2(s-\mu)}\right] \frac{1}{(s-\mu)^{\nicefrac{3}{2}}} &\ 0 < \mu < s < \infty \\
%                             0                                                                                                        &\ sinon
%                         \end{cases}
% \end{align}
% La distribution est le plus souvent exprimée sous la forme d’une transformée de Fourier:
% \begin{equation}\label{eq:fourier_levy}
%     \mathcal{F}(k) = \exp(-\alpha\mathopen{|}k\mathclose{|}^{\beta}) \qquad  0 < \beta \leq 2
% \end{equation}
% Il existe des cas spéciaux où la transformée inverse de Fourier correspond à une distribution
% normale ($\beta = 2$), ou une distribution de Gauchy ($\beta = 1$).

% La forme analytique de la \emph{distribution de Lévy stable et symétrique} (symmetrical Lévy stable distribution with index $\beta$) peut
% s’exprimer sous la forme suivante \parencite{Gutowski2001}):
% \begin{equation}\label{eq:dist_levy}
%     L(s) = \frac{1}{\pi} \int_{0}^{\infty} \cos(k s)\exp(-\alpha k^{\beta}) dk \qquad  0 < \beta \leq 2, \quad \alpha > 0
% \end{equation}
% L’intégrale \eqref{eq:dist_levy} est souvent approximée à une simple loi de puissance de la forme:
% \begin{equation}\label{eq:power_levy}
%     L(s) \sim \mathopen{|}s\mathclose{|}^{-1-\beta} \qquad  0 < \beta \leq 2, \quad s \to \infty
% \end{equation}
% Pour rappel, une distribution est dite stable\munsure{\url{https://en.wikipedia.org/wiki/Stable_distribution}} si la
% combinaison linéaire de deux échantillons, a la même distribution indépendamment du pas minimum ($\nu$) et du facteur
% d’échelle ($\gamma$).
% Il est important de noter que son espérance et sa variance sont infinies.
% D’après \eqref{eq:dist_levy}, la distribution peut être estimée seulement quand $s$ est grand:
% \begin{equation}
%     L(s) \approx \frac{\mathbf{\Gamma}(\beta)sin(\pi\frac{\beta}{2})}{\pi\mathopen{|}s\mathclose{|}^{1+\beta}}, \qquad s \to \infty
% \end{equation}
% $\mathbf{\Gamma}$ représentant la fonction Gamma qui est l’extension analytique de la fonction factoriel pour
% les complexes et les réels ($\mathbf{\Gamma}(\beta) = (\beta -1)!, \quad \beta\in \mathbb{N}$) et est définie par:
% \begin{equation}
%     \mathbf{\Gamma}(\beta) = \int_{0}^{\infty} t^{\beta-1}e^{-t} dt
% \end{equation}
% L’algorithme de Mantegna \parencite{Mantegna19944677}) est souvent utilisé pour générer aléatoirement des nombres
% dont la densité de probabilité est proche de celle d’une distribution de Lévy stable et symétrique (le pas peut
% être positif ou négatif).
% La longueur du pas est alors calculé à l’aide de deux distributions normales de la manière suivante:
% \begin{equation}\label{eq:step_len}
%     s = \frac{u}{\mathopen{|}v\mathclose{|}^{\nicefrac{1}{\beta}}}, \qquad u \sim \mathcal{N}(0, \sigma_{u}^{2}), \quad v \sim \mathcal{N}(0, \sigma_{v}^{2})
% \end{equation}
% avec:
% \begin{equation}\label{eq:sigmas}
%     \sigma_{u} = \left[ \frac{\mathbf{\Gamma}(1+\beta)\sin(\pi\frac{\beta}{2})}%
%                              {\mathbf{\Gamma} \left[\frac{(1+\beta)}{2}\beta 2^{\frac{(\beta-1)}{2}}\right]}\right]^{\nicefrac{1}{\beta}},%
%     \qquad \sigma_{v} = 1
% \end{equation}

% Équations~\eqref{eq:step_len}, \eqref{eq:sigmas} il est ainsi possible de définir une longueur de pas aléatoirement et qui
% grâce au caractère symétrique de la distribution peut être positif ou négatif.
% Fig.~\ref{fig:levy_length} permet de mettre en évidence ce comportement avec 100 tirages aléatoires
% obéissant à une distribution de Lévy, et, Fig.~\ref{fig:levy_flight} montre le chemin parcouru durant
% une séquence de vol de Lévy. Il est intéressant de noter que le comportement de la
% vol de Lévy peut être assimilé à une intensification (forte probabilité de générer un petit saut)
% couplé à une exploration (faible probabilité de générer un saut important).

% De plus la variance d’un vol de Lévy~\eqref{eq:variance_brownien_levy} augmente plus rapidement que celle d’un mouvement
% brownien\footnote{marche aléatoire obéissent à une distribution normale/gaussienne}
% permettant aux longueurs des marches aléatoires d’être plus importante (Fig.~\ref{fig:levy_vs_gaussian}).
% Cette dernière caractéristique permet d’augmenter l’exploration mais aussi d’éviter
% de se coincer dans un optimum local contrairement à un mouvement brownien. En effet
% bien que statistiquement similaires, la densité de probabilité dans l’espace du mouvement brownien
% est fortement concentré localement contrairement à l’effet d’île caractéristique du vol de Lévy.
% C’est une caractéristique aussi partagée par les marche aléatoire dite sous-diffuse \eqref{eq:distance_moy}.

% La distance moyenne \eqref{eq:distance_moy} d’une marche aléatoire est fonction du temps $t$. La diffusion est
% dite \emph{améliorée} pour $v > 1$ \parencite{Gutowski2001}):
% \begin{equation}\label{eq:distance_moy}
%     \langle RMS^{2}(t) \rangle = Dt^{v}, \qquad \text{D étant la constante de diffusion}\\\\
% \end{equation}
% La variance ($Var(\mathbf{X}) \sim \langle RMS^{2}\rangle$) pour un mouvement Brownien
% et un vol de Lévy est définie par:
% \begin{align}\label{eq:variance_brownien_levy}
%     \begin{split}
%         \text{Mouvement brownien }  \quad \Rightarrow \quad & Var(\mathbf{X}) \sim \ t\\
%         \text{Vol de Lévy }         \quad \Rightarrow \quad & Var(\mathbf{X}) \sim \ t^{3-\beta}, \qquad 0 < \beta \leq 2
%     \end{split}
% \end{align}
